{
  "hash": "64cf2b8a0b1163e7c9d06a86b155b3c4",
  "result": {
    "engine": "jupyter",
    "markdown": "# Testing Scripts in Containers Interactively\n\nYou might wonder how to test a script will work on a worker. That's what interactive shells are for.\n\nWith an interactive shell, we open a bash prompt on a worker. We can directly test our script out on the worker and debug it.\n\nEven if you aren't going to use Apptainer in your work, I recommend getting into an interactive shell in a container at least once or twice to learn about the container filesystem and how you connect it to the external filesystem.\n\n## Learning Objectives\n\nAfter reading this, you will be able to:\n\n- **Articulate** reasons why interactive shells make testing code easier\n- **Open** an interactive shell on a worker node\n- **Open** an interactive shell on a Docker Container\n- **Articulate** how to connect container filesystems to the external filesystem\n\n## Grabbing an interactive shell\n\nWhen you're testing code that's going to run on a worker node, you need to be aware of what the worker node sees. \n\nIt's also important in estimating how long our tasks are going to run since we can test how long a task runs for a representative dataset. \n\nConfusingly, depending on On a SLURM system, the way to open interactive shells on a node has changed. Check your version first:\n\n```bash\nsrun --version\n```\n\nIf the version is past 20.11, we can open an interactive shell on a worker with `salloc`. \n\n:::{.callout-note}\n## For FH Users: `grabnode`\n\nOn the FH system, we can use a command called `grabnode`, which will let us request a node. It will ask us for our requirements (numbers of cores, memory, etc.) for our node. \n\n\n```{bash}\n#| eval: false\ntladera2@rhino01:~$ grabnode\n```\n\n\n`grabnode` will then ask us for what kind of instance we want, in terms of CPUs, Memory, and GPUs. Here, I'm grabbing a node with 8 cores, 8 Gb of memory, using it for 1 day, and no GPU.\n\n```\nHow many CPUs/cores would you like to grab on the node? [1-36] 8\nHow much memory (GB) would you like to grab? [160] 8\nPlease enter the max number of days you would like to grab this node: [1-7] 1\nDo you need a GPU ? [y/N]n\n\nYou have requested 8 CPUs on this node/server for 1 days or until you type exit.\n\nWarning: If you exit this shell before your jobs are finished, your jobs\non this node/server will be terminated. Please use sbatch for larger jobs.\n\nShared PI folders can be found in: /fh/fast, /fh/scratch and /fh/secure.\n\nRequesting Queue: campus-new cores: 8 memory: 8 gpu: NONE\nsrun: job 40898906 queued and waiting for resources\n```\n\nAfter a little bit, you'll arrive at a new prompt:\n\n```\n(base) tladera2@gizmok164:~$\n```\n\nIf you're doing interactive analysis that is going to span over a few days, I recommend that you use `screen` or `tmux`.\n:::\n\n:::{.callout}\n## Remember `hostname`\nWhen you are doing interactive analysis, it is easy to forget in which node you're working in. Just as a quick check, I use `hostname` to remind myself whether I'm in `rhino`, `gizmo`, or within an apptainer container.\n:::\n\n## Containers\n\nIn this section, we talk about testing scripts in a container using `apptainer`. We use `apptainer` (formerly Singularity) in order to run Docker containers on a shared HPC system. This is because Docker itself requires root-level privileges, which is not secure on shared systems.\n\nIn order to do our testing, we'll first pull the Docker container, map our bind point (so our container can access files outside of its file sytem), and then open \n\n### Pulling a Docker Container\n\nLet's pull a docker container from the Docker registry. Note we have to specify `docker://` when we pull the container, because Apptainer has its own internal format.\n\n```\napptainer pull docker://biocontainers/samtools:v1.9-4-deb_cv1\n```\n\nI recommend that you \n\n## Opening a Shell in a Docker Container\n\nWhen you're getting started, opening a shell using Apptainer can help you test out things like filepaths and how they're accessed in the container. It's hard to get an intuition for how this works until you can see the limited view from the container.\n\nYou can open an Apptainer shell in a container using `apptainer shell`. Remember to use `docker://` before the container name. For example:\n\n\n```{bash}\n#| eval: false\napptainer shell docker://biocontainers/samtools:v1.9-4-deb_cv1\n```\n\n\nThis will open a Bash shell in the container. Once you're in the container, you can test code, especially seeing whether your files can be seen by the container (see @sec-bindpaths). 90% of the issues with using Docker containers has to do with bind paths, so we'll talk about that next.\n\nOnce you're in the shell, you can take a look at where `samtools` is installed:\n\n\n```{bash}\n#| eval: true\nwhich samtools\n```\n\n\n```\n\n```\n\n### Testing out bind paths in containers {#sec-bindpaths}\n\nOne thing to keep in mind is that every container has its own filesystem. One of the hardest things to wrap your head around for containers is how their filesystems work, and how to access files that are outside of the container filesystem. We'll call any filesystems outside of the container *external filesystems* to make the discussion a little easier.\n\nBy default, the containers have access to your current working directory. We could make this where our scripts live (such as `/home/tladera2/`), but because our data is elsewhere, we'll need to specify that location (`/fh/fast/mylab/`) as well.\n\nThe main mechanism we have in Apptainer to access the external filesystem are *bind paths*. Much like mounting a drive, we can bind directories from the external filesystem using these bind points. \n\n\n```{mermaid}\nflowchart LR\n   A[Container Filesystem\\n/mydata/]--write-->C(\"--bind /fh/fast/mydata/:/mydata/\")\n   C --read--> A\n   B[\"External Directory\\n/fh/fast/mydata/\"] \n   B --read--> C\n   C --write--> B\n```\n\n\nI think of bind paths as \"tunnels\" that give access to particular folders in the external filesystem. Once the tunnel is open, we can access data files, process them, and save them using the bind path.\n\nSay my data lives in `/fh/fast/mydata/`. Then I can specify a bind point in my `apptainer shell` and `apptainer run` commands.\n\nWe can do this with the `--bind` option:\n\n\n```{bash}\n#| eval: false\napptainer shell --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1\n```\n\n\nNote that the bind syntax doesn't have the trailing slash (`/`). That is, note that it is:\n\n```\n--bind /fh/fast/mydata: ....\n```\nRather than\n\n```\n--bind /fh/fast/mydata/: ....\n```\n\nNow our `/fh/fast/mydata/` folder will be available as `/mydata/` in my container. We can read and write files to this bind point.\n\n:::{.callout}\n## WDL makes this way easier\n\nA major point of failure with apptainer scripting is when our scripts aren't using the right bind points. \n\nThis is one reason we recommend writing WDL Workflows and a workflow engine (such as Cromwell) to run your workflows, since you don't have to worry about them, because they are handled by the workflow engine.\n:::\n\n### Testing in the Apptainer Shell\n\nOk, now we have a bind point, so now we can test our script in the shell. For example, we can see if we are invoking `samtools` in the correct way and that our bind points work.\n\n\n```{bash}\n#| eval: false\nsamtools view -c /mydata/my_bam_file.bam > /mydata/bam_counts.txt\n```\n\n\nTrying out scripts in the container is the best way to understand what the container can and can't see.\n\n### Exiting the container when you're done\n\nYou can `exit`, like any shell you open. You should be out of the container. Confirm by using `hostname` to make sure you're out of the container.\n\n### More Info\n- [Carpentries Section on Apptainer Paths](https://hsf-training.github.io/hsf-training-singularity-webpage/07-file-sharing/index.html) - this is an excellent resource if you want to dive deeper into undestanding container filesystems and bind points.\n- [More about bind paths and other options](https://apptainer.org/docs/user/main/bind_paths_and_mounts.html).\n\n",
    "supporting": [
      "interactive_shell_files"
    ],
    "filters": [],
    "includes": {}
  }
}