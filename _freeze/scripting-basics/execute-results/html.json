{
  "hash": "aff1e23cffa6e581383d77d68b1d6f4d",
  "result": {
    "engine": "jupyter",
    "markdown": "# Shell Scripting Basics {#sec-script} \n\n\n## Learning Objectives\n\n1. **Utilize** positional *arguments* to generalize our scripts\n1. **Articulate** the *three streams* of a command line utility\n1. **Define** variables for use in a bash script  \n1. **Iterate** a script over a set of files using `xargs` loops\n1. **Wrap** executables and scripts in R/Python into a Bash script\n\n\n## Review of Bash scripting\n\nBash scripting is often referred to as a useful \"glue language\" on the internet. Although a lot of functionality can be covered by both JavaScript and Python, bash scripting is still very helpful to know.\n\nWe are going to cover Bash scripting because it is the main shell that is available to us on HPC machines, which are Ubuntu-based.\n\nWe will be using Bash scripts as \"glue\" for multiple applications in HPC computing, including:\n\n1. **Wrapping scripts** from other languages such as R or Python so we can run them using `dx run` on a app such as Swiss Army Knife\n1. **Naming** outputs according to file input names\n1. **Specifying inputs and outputs** in a workflow built by Workflow Description Language (WDL).\n\nAs you can see, knowing Bash is extremely helpful when running jobs on HPC.\n\n## Our first script with positional arguments {#sec-positional}\n\nSay we have [`samtools`](http://www.htslib.org/doc/samtools-stats.html) installed on our own machine. Let's start with a basic script and build from there. We'll call it  `sam_run.sh`. With `nano`, a text editor, we'll start a very basic bash script and build its capabilities out.\n\n\n```{bash}\n#| eval: false \n#| filename: scripting-basics/sam_run.sh\n#!/bin/bash/\nsamtools stats $1 > $2\n```\n\n\nLet's take a look at the command that we're running first. We're going to run `samtools stats`, which will give us statistics on an incoming `bam` or `sam` file and save it in a file. We want to be able to run our script like this:\n\n\n```{bash}\n#| eval: false\nbash sam_run my_file.bam out_stats.txt\n```\n\n\nWhen we run it like that, `sam_run.sh` will run `samtools stat` like this:\n\n\n```{bash}\n#| eval: false\nsamtools stats my_file.bam > out_stats.txt\n```\n\n\nSo what's going on here is that there is some substitution using common arguments. Let's look at these.\n\n### Positional Arguments such as `$1`\n\nHow did the script know where to substitute each of our arguments? It has to do with the argument variables. Arguments (terms that follow our command) are indexed starting with the number 1. We can access the value at the first position using the special variable `$1`. \n\nNote that this works even in quotes.\n\nSo, to unpack our script, we are substituting our first argument for the `$1`, and our second argument for the `$2` in our script. \n\n:::{.callout-note}\n## Test yourself\n\nHow would we rewrite `sam_run.sh` if we wanted to specify the output file as the first argument and the bam file as the second argument?\n\n\n```{bash}\n#| eval: false\n#| filename: scripting-basics/sam_run.sh \n#!/bin/bash/\nsamtools stats $1 > $2\n```\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Answer\n\nFor this script, we would switch the positions of `$1` and `$2`.\n\n\n```{bash}\n#| eval: false\n#!/bin/bash/\nsamtools stats $2 > $1\n```\n\n\nAnd we would run `sam_run.sh` like this:\n\n\n```{bash}\n#| eval: false\nbash sam_run.sh my_file.bam out_stats.txt\n```\n\n:::\n\n### What about named arguments in my script?\n\nSee @sec-named for more info.\n\n\n\n\n## Using pipes: STDIN, STDOUT, STDERR\n\nWe will need to use pipes to chain our commands together. Specifically, we need to take a command that generates a list of files on the platform, and then spawns individual jobs to process each file. For this reason, understanding a little bit more about how pipes (`|`) work in Bash is helpful.\n\nIf we want to understand how to chain our scripts together into a pipeline, it is helpful to know about the different streams that are available to the utilities.\n\n:::{#fig-std}\n\n```{mermaid}\ngraph LR\n  A(STDIN) --> E[run_samtools.sh]\n  E --> B(STDOUT)\n  E --> C(STDERR)\n```\n\nInputs/outputs to a script\n:::\n\nEvery script has three streams available to it: Standard In (STDIN), Standard Out (STDOUT), and Standard Error (STDERR) (@fig-std).\n\nSTDIN contains information that is directed to the input of a script (usually text output via STDOUT from another script).\n\nWhy do these matter? To work in a Unix pipeline, a script must be able to utilize STDIN, and generate STDOUT, and STDERR.\n\nSpecifically, in pipelines, STDOUT of a script (here it's `run_samtools`) is directed into STDIN of another command (here `wc`, or word count)\n\n:::{#fig-pipe}\n\n```{mermaid}\ngraph LR\n  E[run_samtools.sh] --> B(STDOUT)\n  B --> F{\"|\"}\n  E --> C(STDERR)\n  F --> D(\"STDIN (wc)\")\n  D --> G[wc]\n```\n\nPiping a script `run_samtools.sh` into another command (`wc`)\n:::\n\nWe will mostly use STDOUT in our bash scripts, but STDERR can be really helpful in debugging what's going wrong. \n\n:::{.callout-note}\n## Why this is important on the platform\n\nWe'll use pipes and pipelines not only in starting a bunch of jobs using batch scripting on our home computer, but also when we are processing files within a job.\n:::\n\n### For more info about pipes and pipelines\n\n<https://swcarpentry.github.io/shell-novice/04-pipefilter/index.html>\n<https://datascienceatthecommandline.com/2e/chapter-2-getting-started.html?q=stdin#combining-command-line-tools>\n\n## Batch Processing Basics: Iterating using `xargs` {#sec-xargs}\n\nA really common pattern is taking a delimited list of files and doing something with them. We can do some useful things such as seeing the first few lines of a set of files, or doing some sort of processing with the set of jobs.\n\n:::{.callout-warning}\n## Don't `xargs` for HPC jobs\n  \nYou might be tempted to use `xargs` with `srun` to work on a bunch of files. It's worth trying once so you can see the mechanics of how jobs are processes.\n\nIn general, I don't recommend it in practice because if you spawn 1000 jobs using `xargs`, there's no real mechanism to terminate that 1000 jobs, except one by one. With `sbatch`, all your jobs in batch mode run as *subjobs*\n\nAgain, this is a good reason to use a workflow runner in your day to day work. You don't have to worry about jobs and subjobs. It takes a little setup, but it will make your life easier in general.\n:::\n\nLet's start out with a list of files:\n\n\n```{bash}\n#| eval: false\nsource ~/.bashrc #| hide_line\nls data/*.sh\n```\n\n\n```\ndata/batch-on-worker.sh\n```\n\nNow we have a list of files, let's look at the first few lines of each of them, and print a separator `---` for each.\n\n\n```{bash}\n#| eval: false\n#| filename: scripting-basics/xargs_example.sh\nsource ~/.bashrc #| hide_line\nls data/*.sh | xargs -I% sh -c 'head %; echo \"\\n---\\n\"'\n```\n\n```\n#!/bash/bin\n\ncmd_to_run=\"ls *.vcf.gz | xargs -I% sh -c \"bcftools stats % > %.stats.txt\"\n\ndx run swiss-army-knife \\\n  -iin=\"data/chr1.vcf.gz\" \\\n  -iin=\"data/chr2.vcf.gz\" \\\n  -iin=\"data/chr3.vcf.gz\" \\\n  -icmd=${cmd_to_run}\n---\ndx find data --name \"*.bam\" --brief\n---\n```\n\n\nLet's take this apart piece by piece.\n\n`xargs` takes an `-I` argument that specifies a placeholder. In our case, we are using `%` as our placeholder in this statement. \n\nWe're passing on each filename from `ls` into the following code:\n\n\n```{bash}\n#| eval: false\nsh -c 'head %; echo \"---\\n\"'\n```\n\n\nThe `sh -c` opens a subshell so that we can execute our command for each of the files in our list. We're using `sh -c` to run:\n\n\n```{bash}\n#| eval: false\n'head %; echo \"---\\n\"'\n```\n\n\nSo for our first file, `01-scripting-basics.qmd`, we are substituting that for `%` in our command:\n\n\n```{bash}\n#| eval: false\n'head hpc-basics.qmd; echo \"---\\n\"'\n```\n\n\nFor our second file, `hpc-basics.qmd`, we would substitute that for the `%`:\n\n\n```{bash}\n#| eval: false\n'head hpc-basics.qmd; echo \"---\\n\"'\n```\n\n\nUntil we cycle through all of the files in our list.\n\n### The Basic `xargs` pattern\n\n:::{#fig-xargs}\n\n```{mermaid}\ngraph LR\n  A[\"ls *.bam\"] --> B{\"|\"} \n  B --> C[\"xargs -I% sh -c\"] \n  C --> D[\"command_to_run %\"]\n```\n\nBasics of using `xargs` to iterate on a list of files\n:::\n\nAs you cycle through lists of files, keep in mind this basic pattern (@fig-xargs):\n\n\n```{bash}\n#| eval: false\nls <wildcard> | xargs -I% sh -c \"<command to run> %\"\n```\n\n\n:::{.callout-note}\n## Test Yourself\n\nHow would we modify the below code to do the following?\n\n1. List only `.json` files in our `data/` folder using `ls`\n1. Use `tail` instead of `head`\n\n\n```{bash}\n#| eval: false\nls *.txt | xargs -I% sh -c \"head %; echo '---\\n'\"\n```\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n## Answer\n\n\n```{bash}\n#| eval: false\nls data/*.json | xargs -I% sh -c \"tail %; echo '---\\n'\"\n```\n\n:::\n\n:::{.callout-note}\n## Why this is important on the platform\n\nWe can use `xargs` to execute small batch jobs on a small number of files. This especially becomes powerful on the platform when we use `ls` to list files in our HPC project.\n\nNote that as we *graduate* to workflow tools like WDL/Nextflow, there are other mechanisms for running jobs on multiple files (such as WDL/Cromwell) that we should move to. \n\nTrust me; you don't want to have to handle iterating through a huge directory and handling when routines give an error, or your jobs get interrupted. Rerunning and resuming failed jobs are what workflow runner tools excel at. \n:::\n\n### For more information\n\n<https://www.baeldung.com/linux/xargs-multiple-arguments>\n\n\n## Variables in Bash Scripts {#sec-bash-variables}\n\nWe've already encountered a placeholder variable, `%`, that we used in running `xargs`. Let's talk about declaring variables in bash scripts and using them using variable expansion. \n\nIn Bash, we can declare a variable by using `<variable_name>=<value>`. Note there are no spaces between the variable (`my_variable`), equals sign, and the value (`\"ggplot2\"`).\n\n\n```{bash}\nmy_variable=\"ggplot2\"\n\necho \"My favorite R package is ${my_variable}\"\n```\n\n\nTake a look at line 3 above. We expand the variable (that is, we substitute the actual variable) by using `${my_variable}` in our `echo` statement.\n\nIn general, when expanding a variable in a quoted string, it is better to use `${my_variable}` (the variable name in curly brackets). This is especially important when using the variable name as part of a string:\n\n\n```{bash}\nmy_var=\"chr1\"\necho \"${my_var}_1.vcf.gz\"\n```\n\n\nIf we didn't use the braces here, like this:\n\n```\necho \"$my_var_1.vcf.gz\"\n```\n\nBash would look for the variable `$my_var_1`, which doesn't exist. So use the curly braces `{}` when you expand variables. It's safer overall. \n\nThere is an alternate method for variable expansion which we will use when we call a *sub-shell* - a shell within a shell, much like in our `xargs` command above. We need to use parentheses `()` to expand them within the sub-shell, but not the top-shell. We'll use this when we process multiple files within a single worker.\n\n### `basename` can be very handy when on workers\n\nIf we are processing a bunch of files on a worker, we need a way to get the bare filename from a `dxfuse` path. We will take advantage of this when we run process multiple files on the worker.\n\nFor example:\n\n```\nbasename /mnt/project/worker_scripts/dx-run-script.sh\n```\n\nThis will return:\n\n```\ndx-run-script.sh\n```\n\nWhich can be really handy when we name our outputs. This command is so handy it is used in WDL. \n\n## Quoting and Escaping Filenames in Bash\n\nOne point of confusion is when do you quote things in Bash? When do you use single quotes (`'`) versus double-quotes (`\"`)? When do you use `\\` to escape characters?\n\nLet's talk about some quoting rules in Bash. I've tried to make things as simplified and  generalized as possible, rather than stating all of the rules for each quote.\n\n1. If you have spaces in a filename, use double quotes (`\"chr 1.bam\"`)\n1. If you have a single quote in the filename, use double quotes to wrap it (`\"ted's file.bam\"`)\n1. Only escape characters when necessary - if you can solve a problem with quotes, use them\n1. If you need to preserve an escaped character, use single quotes\n\nLet's go over each of these with an example.\n\n### If you have spaces in a filename, use double quotes (Most common)\n\nFor example, if your filename is `chr 1 file.bam`, then use double quotes in your argument\n\n```\nsamtools view -c \"chr 1 file.bam\"\n```\n\n### If you have a single quote in the name, use double quotes to wrap it (less common)\n\nSay you have a file called `ted's new file.bam`. This can be a problem when you are calling it, especially because of the single quote.\n\nIn this case, you can do this:\n\n```\nsamtools view -c \"ted's new file.bam\"\n```\n\n### Only escape characters when necessary (less common)\n\nThere are a number of special characters (such as Tab, and Newline) that can be specified as escape characters. In double quotes, characters such as `$` are signals to Bash to expand or evaluate code. \n\nSay that someone had a `$` in their file name such as `Thi$file is money.bam`\n\nHow do we refer to it? We can escape the character with a backslash `\\`:\n\n```\nsamtools view -c \"Thi\\$file is money.bam\"\n```\nThe backslash is a clue to Bash that we don't want variable expansion in this case. Without it, bash would look for a variable called `$file`. \n\n### If you need to preserve an escaped character, use single quotes (least common)\n\nThis is rarely used, but if you need to keep an escaped character in your filename, you can use single quotes. Say we have a filename called `Thi\\$file.bam` and you need that backslash in the file name (btw, please don't do this), you can use single quotes to preserve that backslash:\n\n```\nsamtools view -c 'Thi\\$file.bam'\n```\n\nAgain, hopefully you won't need this.\n\n### For More Info\n\n<https://www.grymoire.com/Unix/Quote.html#uh-3>\n\n:::{.callout-note}\n## What about backticks?\n\nBackticks (`` ` ``) are an old way to do command evaluation in Bash. For example, if we run the following on the command-line:\n\n```\necho \"there are `ls -l | wc -l` files in this directory\"\n```\nWill produce:\n\n```\nthere are       36 files in this directory\n```\n\nTheir use is deprecated, so you should be using `$()` in your command evaluations instead:\n\n```\necho \"there are $(ls -l | wc -l) files in this directory\"\n```\n:::\n\n:::{.callout-note}\n## What about X use case?\n\nThere are a lot of rules for Bash variable expansion and quoting that I don't cover here. I try to show you a way to do things that work in multiple situations on the platform.\n\nThat's why I focus on double quotes for filenames and `${}` for variable expansion in general. They will work whether your Bash script is on the command line or in an App, or in WDL. \n\n:::\n\n\n## What you learned in this chapter\n\nWhew, this was a whirlwind tour. Keep this chapter in mind when you're working with the platform - the bash programming patterns will serve you well. We'll refer to these patterns a lot when we get to doing more bioinformatics tasks on the platform.\n\n- Setting up bash scripts with positional arguments\n- Iterating over a list of files using `xargs`\n- How to use bash variables and variable expansions\n\n",
    "supporting": [
      "scripting-basics_files"
    ],
    "filters": [],
    "includes": {}
  }
}