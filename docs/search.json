[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bash for HPC",
    "section": "",
    "text": "1 Introduction\nBash scripting is an essential skill in bioinformatics that we often expect bioinformaticians to have automatically learned. I think that this underestimates the difficulty of learning and applying Bash scripting.\nThis is a book that is meant to bring you (a budding bioinformaticist) beyond the foundational shell scripting skills learned from a shell scripting course such as the Software Carpentries Shell Course.\nSpecifically, this book shows you a path to get started with processing data on a High Performance Computing cluster, and setting you on the road to making a reproducible workflow using WDL.\nOur goal is to showcase the “glue” skills that help you do bioinformatics reproducibly on a High Performance Computing Cluster.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#why-bash",
    "href": "index.html#why-bash",
    "title": "Bash for HPC",
    "section": "1.1 Why Bash?",
    "text": "1.1 Why Bash?\nBash is used as the default shell for many different bioinformatics containers and applications. So writing bash scripts can help you in many different situations where you need to automate a series of steps.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#learning-objectives-for-this-book",
    "href": "index.html#learning-objectives-for-this-book",
    "title": "Bash for HPC",
    "section": "1.2 Learning Objectives for this Book",
    "text": "1.2 Learning Objectives for this Book\nAfter reading and doing the exercises in this book, you should be able to:\n\nArticulate basic HPC architecture concepts and why they’re useful in your work\nUtilize basic SLURM commands to understand the architecture of your HPC cluster\nApply bash scripting to your own work\nLeverage bash scripting to execute jobs on HPC\nExecute batch processing of multiple files in a project\nManage software dependencies reproducibly using container-based technologies such as Docker or environment modules",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-is-not-covered",
    "href": "index.html#what-is-not-covered",
    "title": "Bash for HPC",
    "section": "1.3 What is not covered",
    "text": "1.3 What is not covered\nThis book is not meant to be a substitute for excellent books such as Data Science on the Command Line. This book focuses on the essential Bash shell skills that will help you on HPC systems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#notes",
    "href": "index.html#notes",
    "title": "Bash for HPC",
    "section": "1.4 Notes",
    "text": "1.4 Notes\nThis is a very opinionated journey through Bash shell scripting, workflow languages, and reproduciblity. This is written from the perspective of a user, especially on HPC systems that utilize SLURM.\nIt is designed to build on each of the concepts in a gradual manner. Where possible, we link to the official HPC documentation.\nAt each step, you’ll be able to do useful things with your data. We will focus on skills and programming patterns that are useful.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Bash for HPC",
    "section": "1.5 Prerequisites",
    "text": "1.5 Prerequisites\nBefore you tackle this book, you should be able to accomplish the following:\n\nOpen and utilize a shell on a remote system using a terminal or SSH tool such as PuTTY. This section from the Missing Semester of your CS Education is very helpful: https://missing.csail.mit.edu/2020/course-shell/\nUtilize and navigate File Paths (both absolute and relative) in a Unix system\n\nWe recommend reviewing a course such as the Software Carpentry course for Shell Scripting before getting started with this book. The Missing Semester of your CS Education is another great introduction/resource.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Bash for HPC",
    "section": "1.6 Contributors",
    "text": "1.6 Contributors\nTBD.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#want-to-be-a-contributor",
    "href": "index.html#want-to-be-a-contributor",
    "title": "Bash for HPC",
    "section": "1.7 Want to be a Contributor?",
    "text": "1.7 Want to be a Contributor?\nThis is the first draft of this book. It’s not going to be perfect, and we need help. Specifically, we need help with testing the setup and the exercises.\nIf you have an problem, you can file it as an issue using this link.\nIn your issue, please note the following:\n\nYour Name\nWhat your issue was\nWhich section, and line you found problematic or wouldn’t run\n\nIf you’re Quarto/GitHub savvy, you can fork and file a pull request for typos/edits.\nJust be aware that this is not my primary job - I’ll try to be as responsive as I can.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Bash for HPC",
    "section": "1.8 License",
    "text": "1.8 License\nBash for HPC by Ted Laderas is licensed under a Creative Commons Attribution 4.0 International License.Based on a work at https://github.com/laderast/bash_for_hpc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html",
    "href": "hpc-basics.html",
    "title": "2  HPC Basics",
    "section": "",
    "text": "2.1 Learning Objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#learning-objectives",
    "href": "hpc-basics.html#learning-objectives",
    "title": "2  HPC Basics",
    "section": "",
    "text": "Define key players in both local computing and HPC\nArticulate key differences between local computing and HPC\nDescribe the sequence of events in launching jobs in the HPC cluster\nDifferentiate local storage from shared storage and articulate the advantages of shared storage.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#important-terminology",
    "href": "hpc-basics.html#important-terminology",
    "title": "2  HPC Basics",
    "section": "2.2 Important Terminology",
    "text": "2.2 Important Terminology\nLet’s establish the terminology we need to talk about HPC computing.\n\nHigh Performance Computing - A type of computing that uses higher spec machines, or multiple machines that are joined together in a cluster. These machines can either be on-premise (also called on-prem), or in the cloud (such as Amazon EC machines, or Azure Batch).\nCluster - a group of machines networked such that users can use one or more machines at once.\nAllocation - a temporary set of one or more computers requested from a cluster.\nToolchain - a piece of software and its dependencies needed to build a tool on a computer. For example, cromwell (a workflow runner), and java.\nSoftware Environment - everything needed to run a piece of software on a brand new computer. For example, this would include installing tidyverse, but also all of its dependencies (R) as well. A toolchain is similar, but might not contain all system dependencies.\nExecutable - software that is available on the HPC.\nShared Filesystem - Part of the platform that stores our files and other objects. We’ll see that these other objects include applets, databases, and other object types.\nSLURM - The workload manager of the HPC. Commands in SLURM such as srun, (see Section 2.4) kick off the processes on executing jobs on the worker nodes.\nInteractive Analysis - Any analysis that requires interactive input from the user. Using RStudio and JupyterLab are two examples of interactive analysis. As opposed to non-interactive analysis, which is done via scripts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#understanding-the-key-players",
    "href": "hpc-basics.html#understanding-the-key-players",
    "title": "2  HPC Basics",
    "section": "2.3 Understanding the key players",
    "text": "2.3 Understanding the key players\nIn order to understand what’s going on with HPC, we will have to change our mental model of computing.\nLet’s contrast the key players in local computing with the key players in HPC.\n\n2.3.1 Key Players in Local Computing\n\n\n\n\n\n\nFigure 2.1: Local Computing\n\n\n\n\nOur Machine\n\nWhen we run an analysis or process files on our computer, we are in control of all aspects of our computer. We are able to install a software environment, such as R or Python, and then execute scripts/notebooks that reside on our computer on data that’s on our computer.\nOur main point of access to either the HPC cluster is going to be our computer.\n\n\n2.3.2 Key Players in HPC\nLet’s contrast our view of local computing with the key players in the HPC cluster (Figure 2.2).\n\n\n\n\n\n\nFigure 2.2: Key Players in HPC\n\n\n\n\nOur Machine - We interact with the platform via the dx-toolkit installed on our machine. When we utilize HPC resources, we request them from our own computer using commands from the dx toolkit.\nHead Node - Although there are many parts, we can treat the HPC cluster as a single entity that we interact with. Our request gets sent to the platform, and given availability, it will grant access to a temporary worker. Also contains project storage.\nWorker Node - A temporary machine that comes from a pool of available machines in the cluster. We’ll see that it starts out as a blank slate, and we need to establish a software environment to run things on a worker.\nShared Filesystem A distributed filesystem that can be seen by all of the nodes in the cluster. Our scripts and data live here.\n\n\n\n2.3.3 Further Reading\n\nWorking on a remote HPC system is also a good overview of the different parts of HPC.\n\n\n\n\n\n\n\nFor Fred Hutch Users\n\n\n\nThe gizmo cluster at Fred Hutch actually has 3 head nodes called rhino (rhino01, rhino02, rhino03) that are high spec machines (70+ cores, lots of memory). You can run jobs on these nodes, but be aware that others may be running jobs here as well.\nThe nodes from gizmo all have names like gizmoj6, depending on their architecture.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#sec-srun",
    "href": "hpc-basics.html#sec-srun",
    "title": "2  HPC Basics",
    "section": "2.4 Sequence of Events of Running a Job",
    "text": "2.4 Sequence of Events of Running a Job\nLet’s run through the order of operations of running a job on the platform. Let’s focus on running an aligner (BWA-MEM) on a FASTQ file. Our output will be a .BAM (aligned reads) file.\nLet’s go over the order of operations needed to execute our job on the HPC cluster (Figure 2.3).\n\n\n\n\n\n\nFigure 2.3: Order of Operations\n\n\n\nA. Start a job using srun to send a request to the platform. In order to start a job, we will need two things: software (samtools), and a file to process from the shared filesystem (not shown). When we use srun, a request is sent to the platform.\nB. Platform requests for a worker from available workers; worker made available on platform. In this step, the head node looks for a set of workers that can meet our needs. Then the computations run on the worker; output files are generated.** Once our app is ready and our file is transferred, we can run the computation on the worker.\nC. Output files transferred back to project storage. Any files that we generate during our computation (53525342.bam) must be transferred back into the shared filesystem.\nWhen you are working with an HPC cluster, especially with batch jobs, keep in mind this order of execution. Being familiar with how the key players interact on the platform is key to running efficient jobs.\n\n2.4.1 Key Differences with local computing\nAs you might have surmised, running a job on the HPC platform is very different from computing on your local computer. Here are a few key differences:\n\nWe don’t own the worker machine, we only have temporary access to it. A lot of the complications of running HPC computations comes from this.\nWe have to be explicit about what kind of machine we want. We’ll talk much more about this in terms of machine types and classifieds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#the-shared-filesystem",
    "href": "hpc-basics.html#the-shared-filesystem",
    "title": "2  HPC Basics",
    "section": "2.5 The Shared Filesystem",
    "text": "2.5 The Shared Filesystem\nClusters often have a shared filesystem to make things easier. These filesystems can be accessed by all the nodes in a cluster and are designed for fast file transfers and reading. One example of a filesystem is Lustre.\nThink about that: it’s like having an SSD attached to all of the nodes. But how does the shared storage work?\nThe filesystem is distributed such that each set of nodes has a relatively fast access to the files on the system. The data itself is sharded, or broken up, and distributed among the storage servers that provide access to the files.\n\n\n\n\n\n\nFor FH Users\n\n\n\nThere are three main filesystems you will probably use:\n\n/home/ - usually where your scripts will live\n/fh/fast/ - Where data lives. You will usually transfer data files over to /fh/temp/ and when you generate results, transfer them from /fh/temp/ back to /fh/fast/\n/fh/temp/ - A temporary filesystem. Don’t store files here long term - mostly use this as a faster system to do computations on.\n\n\n\n\n2.5.1 Further Reading\n\nTransferring Files is a nice overview of the ways to transfer files to and from a remote system.\nSciWiki: Permissions - understanding the file permission.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HPC Basics</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#scattering-distribute-the-work",
    "href": "hpc-basics.html#scattering-distribute-the-work",
    "title": "2  HPC Basics",
    "section": "2.7 Scattering: Distribute the Work",
    "text": "2.7 Scattering: Distribute the Work\n\n\n\n\n\n\nFigure 2.4: The scattering process\n\n\n\nSo far, everything we’ve seen so far can be run on a single computer. In the cluster, we have access to higher spec’ed machines, but using the cluster in this way doesn’t take advantage of the efficiency of distributed computing, or dividing the work up among multiple worker nodes.\nWe can see an example of this in Figure 2.4. In distributed computing, we break our job up into smaller parts. One of the easiest way to do this is to split up a list of files (file1.bam, file2.bam, file3.bam) that we need to process, process each file separately on a different node, and then bring the results back together. Each node is only doing part of the work, but because we have multiple nodes, it is getting done 3 times faster.\nYou can orchestrate this process yourself with tools such as sbatch, but it is usually much easier to utilize workflow runners such as Cromwell/PROOF (for .wdl files) or Nextflow (for .nf files), because they automatically handle saving the results of intermediate steps.\nTrust me, it is a little more of a learning curve to learn Cromwell or Nextflow, but once you know more about it, the automatic file management and node management makes it much easier in the long run.\n\n\n\n\n\n\nNodes versus CPUs\n\n\n\nOne thing that confused me was understanding the difference between requesting a system with multiple cores versus requesting multiple nodes.\nCores roughly correspond to processors, so a 24-core allocation is a single node that has 24 CPUs.\nNodes correspond to machines - so a 24 node allocation is 24 machines.\nThe reason why this is important is that you use them differently - we use scatter to utilize a 24 node allocation, whereas we can use multicore packages such as {parallel} and mcapply() to utilize a multi-core system.\nIn general, scatter over multiple nodes is handled by sbatch or your workflow runner.\n\n\n\n\n\n\n\n\nFor FH Users: Running Workflows\n\n\n\nAt Fred Hutch, we have two main ways to run workflows on gizmo: Cromwell and NextFlow. Cromwell users have a nifty UI to run their workflows called PROOF.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HPC Basics</span>"
    ]
  },
  {
    "objectID": "intro-unix.html",
    "href": "intro-unix.html",
    "title": "4  Everything about Unix/Linux they didn’t teach you",
    "section": "",
    "text": "4.1 Learning Objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "intro-unix.html#learning-objectives",
    "href": "intro-unix.html#learning-objectives",
    "title": "4  Everything about Unix/Linux they didn’t teach you",
    "section": "",
    "text": "Setup your terminal to connect to an HPC system\nFind and set environment variables in an HPC system\nModify your $PATH variable to include the path to an executable\nUse which to identify which version of an executable you’re using\nExplain built-in utilities that are useful in your work",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "intro-unix.html#terminal-setup",
    "href": "intro-unix.html#terminal-setup",
    "title": "4  Everything about Unix/Linux they didn’t teach you",
    "section": "4.2 Terminal setup",
    "text": "4.2 Terminal setup\nIf you are on Linux/Mac, you’ll be working with the terminal. On Windows, you’ll need a terminal program such as PuTTY to connect to the remote servers.\nIn our examples, we’re going to be connecting to the Fred Hutch servers rhino and the associated cluster, gizmo.\n\n\n\n\n\n\nDon’t Forget the VPN!\n\n\n\nMany HPC systems are behind an organization’s VPN, so you’ll need a VPN client like Cisco Secure Client to get into your VPN.\nFH Users: after connecting through the Fred Hutch VPN you’ll connect to rhino to gain access to the HPC system.\n\n\n\n\n\n\n\n\nA Little More Advanced\n\n\n\nIf you are on Windows, you can install Windows Subsystem for Linux, and specifically the Ubuntu distribution. That will give you a command-line shell that you can use to interact with the remote server. I prefer this route, but PuTTY works great as well.\n\n\nOn your machine, I recommend using a text editor to edit the scripts in your remote shell. Good ones include Visual Studio Code (VS Code), or built in editors such as nano. You can use VSCode to edit scripts remotely using the SSH extension. Editing scripts remotely like this may be more comfortable for you. Note that if you are on a Windows machine that is remotely administered, you will need to contact the admins to enable the OpenSSH extension in Windows for it to work.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "intro-unix.html#where-am-i",
    "href": "intro-unix.html#where-am-i",
    "title": "3  Everything about Unix/Linux they didn’t teach you",
    "section": "3.3 Where Am I?",
    "text": "3.3 Where Am I?\nOne of the confusing things about working on HPC is that sometimes you have a shell open on the head node, but oftentimes, you are on a worker node.\nYour totem for telling which node you’re in is hostname, which will give you the host name of the machine you’re on.\nFor example, if I used grabnode to grab a gizmo node for interactive work, I can check which node I’m in by using:\nhostname\ngizmok164\nIf you’re confused about which node you’re in, remember hostname. It will save you from making mistakes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "intro-unix.html#navigating-the-unixlinux-filesystem",
    "href": "intro-unix.html#navigating-the-unixlinux-filesystem",
    "title": "3  Everything about Unix/Linux they didn’t teach you",
    "section": "3.4 Navigating the Unix/Linux filesystem",
    "text": "3.4 Navigating the Unix/Linux filesystem\nWe’ll start out our Unix/Linux journey talking about the filesystem, which has some quirks we need to be aware of.\n\n\n\n\n\n\nFH users: the main filesystems\n\n\n\nWhen working on the HPC, there are three filesystems you should consider:\n\n/home/ - The home filesystem. Your scripts can live here\n/fh/fast/ (also known as fast) - Research storage. Raw files and processed results should live here.\n/fh/temp/ (also known as temp) - The temporary filesystem. This filesystem is faster to access for gizmo nodes on the cluster, so files can be copied to for computation. The output files you generate should be moved back into an appropriate folder on /fh/fast/. Note that files on /fh/temp/ will be deleted after 30 days.\n\nBelow is a a diagram with one way to work with these multiple filesystems.\n\nWe transfer the raw files to be processed from /fh/fast/ to our directory /fh/temp/. For example, a set of .bam files.\nWe run our scripts from /home/, on the raw files in /fh/temp/ and produce results in /fh/temp/\nWe transfer our results from /fh/temp/ to /fh/fast/\n\n\n\n\n\n\ngraph LR\nA[\"Home Directory\\n/home/tladera2/\\nScripts\"] --\"2. run scripts\"--&gt; C\nB[\"Fast\\n/fh/fast/tladera2\\nResearch Data\"] --\"1. transfer\\nraw files\"--&gt; C\nC[\"Temp\\n/fh/temp/tladera2\"] --\"3. transfer\\nresults\"--&gt; B\n\n\n\n\n\n\nYour main mechanism for getting files to and from fast and scratch is Motuz, which is a GUI based file transfer utility. One of the advantages of Motuz is that it supports file resuming in case of disconnection, avoiding having to reupload the whole batch of files over again.\nMotuz also supports file transfers between other filesystems at FH, including the FH Amazon S3 bucket. Highly recommended.\n\n\n\n3.4.1 Going /home: ~/\nThere is one important shortcut you should always remember: ~/ is shorthand for your own home directory.\nDepending on the linux distribution, this can be a different location. On the FH filesystem, when I use ~/, it maps to:\n/home/tladera2\nThe home directory is also important because it is where your configuration files live, such as .bashrc (see below).\nDepending on how you work, you may want to store your scripts and workflows in /home/. Some people prefer to keep their scripts, data, and results in a single folder. They usually have a structure like this:\n\n\n\n3.4.2 Absolute versus relative paths\nAbsolute paths contain all the information needed to find a file in a file system from the root / directory. For example, this would be an absolute path:\n/fh/fast/laderas_t/immuno_project/raw_data/tcr_data.fasta\nAbsolute paths always start with /, because that is the root directory, where all the top folders and files live.\nRelative paths break up an absolute path into two pieces of information: 1) your current directory and 2) the path relative to that directory. Relative paths are really helpful because things don’t break when you move your folder or files.\nIf my current path the directory /fh/fast/laderas_t/immuno_project/, then the relative path to that same file would be:\nraw_data/tcr_data.fasta\nNote that this relative path does not start with a /, because our current directory isn’t the root directory. Relative paths are incredibly useful when scripting in a reproducible manner, such as using project-based workflows to process files in a single folder.\n\n\n\n\n\n\nProject/folder based workflows\n\n\n\nOn a particular machine, using absolute paths is safe. However, you do this at the cost of portability - code that you write on one machine may not run on another.\nIf you ever anticipate doing the analysis on a separate machine, using project structures with relative paths is the safest.\nFor example, here’s one example of putting everything into a single folder:\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nWhen we run run_workflow.sh, it will run bowtie on all of the files in data/, and save them in results/, resulting in the following updated structure.\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n│   ├── chr1.bam\n│   ├── chr2.bam\n│   └── chr3.bam\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\n\n\nYou may have seen relative paths such as ../another_directory/ - the .. means to go up a directory in the file hierarchy, and then look in that directory for the another_directory/ directory. I try to avoid using relative paths like these.\nIn general for portability and reproducibility, you will want to use relative paths within a directory, and avoid using relative paths like ../../my_folder, where you are navigating up. In general, use relative paths to navigate down.\n\n\n3.4.3 Keep Everything in a Folder\nWe need to talk about code and data organization. For the FH system, we have a /home/ directory, and if we have generated research data, a /fh/fast/ directory. If we want our scripts to live in /home/ and our data is in /fh/temp/, we’ll need to\nIdeally, we want to make the naming conventions of our code and our data as similar as possible.\nThe requirements of bioinformatics analysis usually require a huge amount of data, so storing data and scripts in a single directory is usually not recommended.\n\n\n3.4.4 Moving Things Around\nA lot of the time, we need to move files between shared filesystems. One filesystem might be good at storage and be backed up on a regular basis, while another filesystem might be better for temporary work on the cluster.\nYou might be familiar with mv, which lets you move files around in Unix. One thing to keep in mind when you’re mving things to a new folder that there is a difference between:\nmv log.txt my_folder   ## renames log.txt to my_folder\nand\nmv log.txt my_folder/  ## moves log.txt to be in my_folder\nThis is one thing that trips me up all the time.\nThis is one situation where using a GUI such as Motuz can be very helpful.\n\n\n3.4.5 Things I always forget: the difference between /home/mydir/ and home/mydir/\nSome things that trip me up all the time. The difference between\n/home/mydir/    #absolute path\nand\nhome/mydir/    #relative path\nThe first one is an absolute path, and the second is a relative path. Your clue is the leading / at the beginning of a path. If you’re getting file not found messages, check to make sure the path is the right format.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "intro-unix.html#environment-variables",
    "href": "intro-unix.html#environment-variables",
    "title": "3  Everything about Unix/Linux they didn’t teach you",
    "section": "3.4 Environment Variables",
    "text": "3.4 Environment Variables\nEnvironment variables are variables which can be seen globally in the Linux (or Windows) system across executables.\nYou can get a list of all set environment variables by using the env command. Here’s an example from my own system:\nenv\nSHELL=/bin/bash\nNVM_INC=/home/tladera2/.nvm/versions/node/v21.7.1/include/node\nWSL_DISTRO_NAME=Ubuntu\nNAME=2QM6TV3\nPWD=/home/tladera2\nLOGNAME=tladera2\n[....]\nOne common environment variable you may have seen is $JAVA_HOME, which is used to find the Java Software Development Kit (SDK). (I usually encounter it when a software application yells at me when I haven’t set it.)\nYou can see whether an environment variable is set using echo, such as\necho $PATH\n/home/tladera2/.local/bin:/home/tladera2/gems/bin:/home/tladera2/.nvm/versions/node/v21.7.1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/ [....]\n\n\n\n\n\n\nEnvironment Variables in Windows\n\n\n\nSince we’re mostly going to be working in a Unix environment, we’re not going to touch on Windows environment variables. However, there is also a $PATH environment variable that you can set.\nI recommend looking at the PowerShell documentation for more information about Windows-specific environment variables\n\n\n\n3.4.1 Setting Environment Variables\nIn Bash, we use the export command to declare an environment variable. For example, if we wanted to declare the environment variable $SAMTOOLS_PATH we’d do the following:\n# works: note no spaces\nexport SAMTOOLS_PATH=\"/home/tladera2/miniconda/bin/\"\nOne thing to note is that spacing matters when you declare environment variables. For example, this won’t declare the $SAMTOOLS_PATH variable:\n# won't work because of spaces\nexport SAMTOOLS_PATH = \"/home/tladera2/miniconda/bin/\"\nAnother thing to note is that we declare environment variables differently than we use them. If we wanted to use SAMTOOLS_PATH in a script, we use a dollar sign ($) in front of it:\n$SAMTOOLS_PATH/samtools view -c $input_file\nIn this case, the value of $SAMTOOLS_PATH will be expanded (substituted) to give the overall path:\n/home/tladera2/miniconda/bin/samtools view -c $input_file\n\n\n3.4.2 A Very Special Environment Variable: $PATH\nThe most important environment variable is the $PATH variable. This variable is important because it determines where to search for software executables (also called binaries). If you have softwware installed by a package manager (such as miniconda), you may need to add the location of your executables to your $PATH.\nWe can add more directories to the $PATH by appending to it. You might have seen the following bit of code in your .bashrc:\nexport PATH=$PATH:/home/tladera2/samtools/\nIn this line, we are adding the path /home/tladera2/samtools/ to our $PATH environment variable. Note that how we refer to the PATH variable is different depending on which side the variable is on of the equals sign.\nTLDR: We declare the variable using export PATH (no dollar sign) and we append to the variable using $PATH (with dollar sign). This is something that trips me up all the time.\n\n\n\n\n\n\nFor FH Users\n\n\n\nIn general, when you use environment modules on gizmo, you do not need to modify your $PATH variable. You mostly need to modify it when you are compiling executables so that the system can find them. Be sure to use which to see where the environment module is actually located:\nwhich samtools\n\n\n\n\n3.4.3 Making your own environment variables\nOne of the difficulties with working on a cluster is that your scripts may be in one filesystem (/home/), and your data might be in another filesystem (/fh/fast/). And it might be recommended that you transfer over files to a faster-access filesystem (/fh/temp/) to process them.\nYou can set your own environment variables for use in your own scripts. For example, we might define a $TCR_FILE_HOME variable:\nexport TCR_FILE_HOME=/fh/fast/my_tcr_project/\nto save us some typing across our scripts. We can use this new environment variable like any other existing environment variable:\n#!/bin/Bash\nexport my_file_location=$TCR_FILE_HOME/fasta_files/\n\n\n3.4.4 .bashrc versus .bash_profile\nOk, what’s the difference between .bashrc and .bash_profile?\nThe main difference is when these two files are sourced. bash_profile is used when you do an interactive login, and .bashrc is used for non-interactive shells.\n.bashrc should contain the environment variables that you use all the time, such as $PATH and $JAVA_HOME for example.\nYou can get the best of both worlds by including the following line in your .bash_profile:\nsource ~/.bashrc\nThat way, everything in the .bashrc file is loaded when you log in interactively.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "intro-unix.html#working-with-shell-scripts",
    "href": "intro-unix.html#working-with-shell-scripts",
    "title": "4  Everything about Unix/Linux they didn’t teach you",
    "section": "4.5 Working with Shell Scripts",
    "text": "4.5 Working with Shell Scripts\n\nNote that I’m only covering bash scripting (hence the name of the book). Each shell has different conventions.\n\nWhen you are writing shell scripts, there’s a few things to know to make them executable.\n\n4.5.1 The she-bang: #!\nIf you’ve looked at a shell script and seen the following:\n#| filename: samcount.sh\n#!/bin/bash\nsamtools view -c $1 &gt; $1.counts.txt\nthe #! is known as a she-bang - it’s a signal to Linux what shell interpreter to use when running the script on the command line.\n\n\n4.5.2 Making things executable: chmod\nNow we have our shell script, we will need to make it executable. We can do this using chmod\nchmod +x samcount.sh\nNow we can run it using:\n./samcount.sh bam_file.bam\nBecause the script is not on our $PATH, then we need to specify the location of the script using ./.\nNote that you can always execute scripts using the bash command, even if they’re not executable for you on your filesystem. You will still need read access.\nbash samcount.sh bam_file.bam\nMuch more info about file permissions is here: Permissions (at the Carpentries)\n\n\n4.5.3 User Access: Groups\nThe groups that you are a member of essentially control access to other files that you don’t own.\nYou can see which groups you are a member of by using groups. For example, on my local Windows Subsystem for Linux filesystem, I am a member of the following groups.\ngroups\ntladera2 adm dialout cdrom floppy sudo audio dip video plugdev netdev\nAs an HPC user, you will usually not have root-level access to the cluster. Again, because it is a shared resource, this is a good thing. The trick is knowing how to install software and add it to your path, or run software containers with new software on a shared system.\n\n\n\n\n\n\nWhy Apptainer and Not Docker?\n\n\n\nWhen we talk more about software environments, we’ll talk about Docker.\nDocker requires root-level access to run processes on a machine. There is a special docker group that has pretty much root-level access.\nOn a shared system such as an HPC cluster, we don’t want to grant such access to individual users.\nEnter Apptainer, which was designed for HPC clusters from the ground up. You can run Docker/Apptainer containers on a shared system without needing root-level access.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "intro-unix.html#useful-utilities",
    "href": "intro-unix.html#useful-utilities",
    "title": "4  Everything about Unix/Linux they didn’t teach you",
    "section": "4.6 Useful Utilities",
    "text": "4.6 Useful Utilities\nThe following section outlines some useful unix utilities that can be very helpful when you’re working in bash. Most of these should be available in HPC systems by default.\n\n4.6.1 Text editors: vim or nano\nIn general, we recommend connecting an editor such as VS Code with the SSH extension to make it easier to edit files. But sometimes you just need to edit a file on the system directly.\nThat’s what nano and vim are for. Of these, nano has the smallest learning curve, since it works like most editors. vim is powerful (especially for searching and substitution), but there is a steep learning curve associated with it.\n\n\n4.6.2 screen or tmux: keep your session open\n\nOftentimes, when you are running something interactive on a system, you’ll have to leave your shell open. Otherwise, your running job will terminate.\nYou can use screen or tmux, which are known as window managers, to keep your sessions open on a remote machine. We’ll talk about screen.\n\nscreen works by starting a new bash shell. You can tell this because your bash prompt will change.\nThe key of working remotely with screen is that you can then request an hpc node.\nFor FH users, you can request a gizmo node using grabnode. We can then check we’re on the gizmo node by using hostname.\nIf we have something running on this node, we can keep it running by detaching the screen session. Once we are detached, we should check that we’re back in rhino by using hostname. Now we can log out and our job will keep running.\n\nIf we need to get back into that screen session, we can use:\nscreen -ls\nTo list the number of sessions:\nThere is a screen on:\n        37096.pts-321.rhino01   (05/10/2024 10:21:54 AM)        (Detached)\n1 Socket in /run/screen/S-tladera2.\nOnce we’ve found the id for our screen session (in this case it’s 37096), we can reattach to the screen session using:\nscreen -r 37096\nAnd we’ll be back in our screen session! Handy, right?\n\n\n\n\n\n\nFor FH Users\n\n\n\nNote that if you logout from rhino, you’ll need to log back into the same rhino node to access your screen session.\nFor example, if my screen session was on rhino01, I’d need to ssh back into rhino01, not rhino02 or rhino03. This means you will need to ssh into rhino01 specifically to get back into your screen session.\n\n\n\n\n4.6.3 The Tab key\nNever underestimate the usefulness of the tab key, which triggers autocompletion on the command line. It can help you complete paths to files and save you a lot of typing.\n\n\n4.6.4 squeue -u &lt;username&gt;\nSometimes you will want to know where you are in the queue of all the other jobs that are in the run queue in SLURM. You can use squeue with -u (username) option to look for your username. For example:\nsqueue -u tladera2",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "running-stuff.html",
    "href": "running-stuff.html",
    "title": "5  Running Executables on HPC",
    "section": "",
    "text": "5.1 A empty environment\nRemember, we can poke around and see whether samtools is installed on the machine using which:\nIf samtools is available, it will give the path. If it isn’t, you will have an empty response.\nSo if we don’t have samtools immediately available, how do we find it on our system? We can use environment modules to load software.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Running Executables on HPC</span>"
    ]
  },
  {
    "objectID": "running-stuff.html#a-empty-environment",
    "href": "running-stuff.html#a-empty-environment",
    "title": "5  Running Executables on HPC",
    "section": "",
    "text": "which samtools",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Running Executables on HPC</span>"
    ]
  },
  {
    "objectID": "running-stuff.html#environment-modules",
    "href": "running-stuff.html#environment-modules",
    "title": "5  Running Executables on HPC",
    "section": "5.2 Environment Modules",
    "text": "5.2 Environment Modules\nBefore you install your own versions of software, it’s important to realize that this problem may be solved for you.\nYour first stop should be looking for environment modules on the HPC. Not all HPCs have these, but if they have them, this should be your first stop to find executables.\nlmod is a system for loading and unloading software modules. It is usually installed on HPCs. The commands all start with module, and there are a number of ones that are useful for you.\n\nmodule avail\nmodule load\nmodule purge",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Running Executables on HPC</span>"
    ]
  },
  {
    "objectID": "running-stuff.html#for-fh-users-modules-benefit-everyone",
    "href": "running-stuff.html#for-fh-users-modules-benefit-everyone",
    "title": "5  Running Executables on HPC",
    "section": "5.3 For FH Users: Modules benefit everyone",
    "text": "5.3 For FH Users: Modules benefit everyone\nIf there is a particular bit of software that you need to run on the FH cluster that’s not there, make sure to request it from SciComp. Someone else probably needs it and so making it known so they can add it as a Environment module will help other people.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Running Executables on HPC</span>"
    ]
  },
  {
    "objectID": "running-stuff.html#running-software-thats-not-available.",
    "href": "running-stuff.html#running-software-thats-not-available.",
    "title": "5  Running Executables on HPC",
    "section": "5.4 Running Software that’s not available.",
    "text": "5.4 Running Software that’s not available.\nIf there is not a module available for our software, then we have a few options, in terms of effort.\n\nUse a Docker container with apptainer that has our software in it\nInstall the binary into our /home directory using conda\nCompile the executable ourselves\n\nFor more information about these different methods, please refer to this article: Why your computational environment is important.\nWe will get more in-depth into containers in the Interactive Shell chapter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Running Executables on HPC</span>"
    ]
  },
  {
    "objectID": "scripting-basics.html",
    "href": "scripting-basics.html",
    "title": "7  Shell Scripting Basics",
    "section": "",
    "text": "7.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Shell Scripting Basics</span>"
    ]
  },
  {
    "objectID": "scripting-basics.html#learning-objectives",
    "href": "scripting-basics.html#learning-objectives",
    "title": "7  Shell Scripting Basics",
    "section": "",
    "text": "Utilize positional arguments to generalize our scripts\nArticulate the three streams of a command line utility\nDefine variables for use in a bash script\n\nIterate a script over a set of files using xargs loops\nWrap executables and scripts in R/Python into a Bash script",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Shell Scripting Basics</span>"
    ]
  },
  {
    "objectID": "scripting-basics.html#review-of-bash-scripting",
    "href": "scripting-basics.html#review-of-bash-scripting",
    "title": "7  Shell Scripting Basics",
    "section": "7.2 Review of Bash scripting",
    "text": "7.2 Review of Bash scripting\nBash scripting is often referred to as a useful “glue language” on the internet. Although a lot of functionality can be covered by both JavaScript and Python, bash scripting is still very helpful to know.\nWe are going to cover Bash scripting because it is the main shell that is available to us on HPC machines, which are Ubuntu-based.\nWe will be using Bash scripts as “glue” for multiple applications in HPC computing, including:\n\nWrapping scripts from other languages such as R or Python so we can run them using dx run on a app such as Swiss Army Knife\nNaming outputs according to file input names\nSpecifying inputs and outputs in a workflow built by Workflow Description Language (WDL).\n\nAs you can see, knowing Bash is extremely helpful when running jobs on HPC.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Shell Scripting Basics</span>"
    ]
  },
  {
    "objectID": "scripting-basics.html#sec-positional",
    "href": "scripting-basics.html#sec-positional",
    "title": "7  Shell Scripting Basics",
    "section": "7.3 Our first script with positional arguments",
    "text": "7.3 Our first script with positional arguments\nSay we have samtools installed on our own machine. Let’s start with a basic script and build from there. We’ll call it sam_run.sh. With nano, a text editor, we’ll start a very basic bash script and build its capabilities out.\n#!/bin/bash/\nsamtools stats $1 &gt; $2\nLet’s take a look at the command that we’re running first. We’re going to run samtools stats, which will give us statistics on an incoming bam or sam file and save it in a file. We want to be able to run our script like this:\nbash sam_run my_file.bam out_stats.txt\nWhen we run it like that, sam_run.sh will run samtools stat like this:\nsamtools stats my_file.bam &gt; out_stats.txt\nSo what’s going on here is that there is some substitution using common arguments. Let’s look at these.\n\n7.3.1 Positional Arguments such as $1\nHow did the script know where to substitute each of our arguments? It has to do with the argument variables. Arguments (terms that follow our command) are indexed starting with the number 1. We can access the value at the first position using the special variable $1.\nNote that this works even in quotes.\nSo, to unpack our script, we are substituting our first argument for the $1, and our second argument for the $2 in our script.\n\n\n\n\n\n\nTest yourself\n\n\n\nHow would we rewrite sam_run.sh if we wanted to specify the output file as the first argument and the bam file as the second argument?\n#!/bin/bash/\nsamtools stats $1 &gt; $2\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor this script, we would switch the positions of $1 and $2.\n#!/bin/bash/\nsamtools stats $2 &gt; $1\nAnd we would run sam_run.sh like this:\nbash sam_run.sh my_file.bam out_stats.txt",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Shell Scripting Basics</span>"
    ]
  },
  {
    "objectID": "scripting-basics.html#using-pipes-stdin-stdout-stderr",
    "href": "scripting-basics.html#using-pipes-stdin-stdout-stderr",
    "title": "7  Shell Scripting Basics",
    "section": "7.4 Using pipes: STDIN, STDOUT, STDERR",
    "text": "7.4 Using pipes: STDIN, STDOUT, STDERR\nWe will need to use pipes to chain our commands together. Specifically, we need to take a command that generates a list of files on the cluster shared filesystem, and then spawns individual jobs to process each file. For this reason, understanding a little bit more about how pipes (|) work in Bash is helpful.\nIf we want to understand how to chain our scripts together into a pipeline, it is helpful to know about the different streams that are available to the utilities.\n\n\n\n\n\n\n\n\ngraph LR\n  A(STDIN) --&gt; E[run_samtools.sh]\n  E --&gt; B(STDOUT)\n  E --&gt; C(STDERR)\n\n\n\n\n\n\n\n\nFigure 7.1: Inputs/outputs to a script\n\n\n\nEvery script has three streams available to it: Standard In (STDIN), Standard Out (STDOUT), and Standard Error (STDERR) (Figure 7.1).\nSTDIN contains information that is directed to the input of a script (usually text output via STDOUT from another script).\nWhy do these matter? To work in a Unix pipeline, a script must be able to utilize STDIN, and generate STDOUT, and STDERR.\nSpecifically, in pipelines, STDOUT of a script (here it’s run_samtools) is directed into STDIN of another command (here wc, or word count)\n\n\n\n\n\n\n\n\ngraph LR\n  E[run_samtools.sh] --&gt; B(STDOUT)\n  B --&gt; F{\"|\"}\n  E --&gt; C(STDERR)\n  F --&gt; D(\"STDIN (wc)\")\n  D --&gt; G[wc]\n\n\n\n\n\n\n\n\nFigure 7.2: Piping a script run_samtools.sh into another command (wc)\n\n\n\nWe will mostly use STDOUT in our bash scripts, but STDERR can be really helpful in debugging what’s going wrong.\n\n\n\n\n\n\nWhy this is important on the Cluster\n\n\n\nWe’ll use pipes and pipelines not only in starting a bunch of jobs using batch scripting on our home computer, but also when we are processing files within a job.\n\n\n\n7.4.1 For more info about pipes and pipelines\nhttps://swcarpentry.github.io/shell-novice/04-pipefilter/index.html https://datascienceatthecommandline.com/2e/chapter-2-getting-started.html?q=stdin#combining-command-line-tools",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Shell Scripting Basics</span>"
    ]
  },
  {
    "objectID": "scripting-basics.html#sec-xargs",
    "href": "scripting-basics.html#sec-xargs",
    "title": "7  Shell Scripting Basics",
    "section": "7.5 Batch Processing Basics: Iterating using xargs",
    "text": "7.5 Batch Processing Basics: Iterating using xargs\nA really common pattern is taking a delimited list of files and doing something with them. We can do some useful things such as seeing the first few lines of a set of files, or doing some sort of processing with the set of jobs.\n\n\n\n\n\n\nDon’t xargs for HPC jobs\n\n\n\nYou might be tempted to use xargs with srun to work on a bunch of files. It’s worth trying once so you can see the mechanics of how jobs are processed.\nIn general, I don’t recommend it in practice because if you spawn 1000 jobs using xargs, there’s no real mechanism to terminate that 1000 jobs, except one by one. With sbatch, all your jobs in batch mode run as subjobs, which means you can terminate the parent job to terminate all of the subjobs.\nAgain, this is a good reason to use a workflow runner in your day to day work. You don’t have to worry about jobs and subjobs. It takes a little setup, but it will make your life easier in general.\n\n\nLet’s start out with a list of files:\nsource ~/.bashrc #| hide_line\nls data/*.sh\ndata/batch-on-worker.sh\nNow we have a list of files, let’s look at the first few lines of each of them, and print a separator --- for each.\n#| filename: scripting-basics/xargs_example.sh\nsource ~/.bashrc #| hide_line\nls data/*.sh | xargs -I% sh -c 'head %; echo \"\\n---\\n\"'\n#!/bash/bin\n\ncmd_to_run=\"ls *.vcf.gz | xargs -I% sh -c \"bcftools stats % &gt; %.stats.txt\"\n\ndx run swiss-army-knife \\\n  -iin=\"data/chr1.vcf.gz\" \\\n  -iin=\"data/chr2.vcf.gz\" \\\n  -iin=\"data/chr3.vcf.gz\" \\\n  -icmd=${cmd_to_run}\n---\ndx find data --name \"*.bam\" --brief\n---\nLet’s take this apart piece by piece.\nxargs takes an -I argument that specifies a placeholder. In our case, we are using % as our placeholder in this statement.\nWe’re passing on each filename from ls into the following code:\nsh -c 'head %; echo \"---\\n\"'\nThe sh -c opens a subshell so that we can execute our command for each of the files in our list. We’re using sh -c to run:\n'head %; echo \"---\\n\"'\nSo for our first file, 01-scripting-basics.qmd, we are substituting that for % in our command:\n'head hpc-basics.qmd; echo \"---\\n\"'\nFor our second file, hpc-basics.qmd, we would substitute that for the %:\n'head hpc-basics.qmd; echo \"---\\n\"'\nUntil we cycle through all of the files in our list.\n\n7.5.1 The Basic xargs pattern\n\n\n\n\n\n\n\n\ngraph LR\n  A[\"ls *.bam\"] --&gt; B{\"|\"} \n  B --&gt; C[\"xargs -I% sh -c\"] \n  C --&gt; D[\"command_to_run %\"]\n\n\n\n\n\n\n\n\nFigure 7.3: Basics of using xargs to iterate on a list of files\n\n\n\nAs you cycle through lists of files, keep in mind this basic pattern (Figure 7.3):\nls &lt;wildcard&gt; | xargs -I% sh -c \"&lt;command to run&gt; %\"\n\n\n\n\n\n\nTest Yourself\n\n\n\nHow would we modify the below code to do the following?\n\nList only .json files in our data/ folder using ls\nUse tail instead of head\n\nls *.txt | xargs -I% sh -c \"head %; echo '---\\n'\"\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nls data/*.json | xargs -I% sh -c \"tail %; echo '---\\n'\"\n\n\n\n\n\n\n\n\n\nWhy this is important on HPC\n\n\n\nWe can use xargs to execute small batch jobs on a small number of files. This especially becomes powerful on the cluster when we use ls to list files in our HPC project.\nNote that as we graduate to workflow tools like WDL/Nextflow, there are other mechanisms for running jobs on multiple files (such as WDL/Cromwell) that we should move to.\nTrust me; you don’t want to have to handle iterating through a huge directory and handling when routines give an error, or your jobs get interrupted. Rerunning and resuming failed jobs are what workflow runner tools excel at.\n\n\n\n\n7.5.2 For more information\nhttps://www.baeldung.com/linux/xargs-multiple-arguments",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Shell Scripting Basics</span>"
    ]
  },
  {
    "objectID": "scripting-basics.html#sec-bash-variables",
    "href": "scripting-basics.html#sec-bash-variables",
    "title": "7  Shell Scripting Basics",
    "section": "7.6 Variables in Bash Scripts",
    "text": "7.6 Variables in Bash Scripts\nWe’ve already encountered a placeholder variable, %, that we used in running xargs. Let’s talk about declaring variables in bash scripts and using them using variable expansion.\nIn Bash, we can declare a variable by using &lt;variable_name&gt;=&lt;value&gt;. Note there are no spaces between the variable (my_variable), equals sign, and the value (\"ggplot2\").\nmy_variable=\"ggplot2\"\n\necho \"My favorite R package is ${my_variable}\"\nMy favorite R package is ggplot2\nTake a look at line 3 above. We expand the variable (that is, we substitute the actual variable) by using ${my_variable} in our echo statement.\nIn general, when expanding a variable in a quoted string, it is better to use ${my_variable} (the variable name in curly brackets). This is especially important when using the variable name as part of a string:\nmy_var=\"chr1\"\necho \"${my_var}_1.vcf.gz\"\nchr1_1.vcf.gz\nIf we didn’t use the braces here, like this:\necho \"$my_var_1.vcf.gz\"\nBash would look for the variable $my_var_1, which doesn’t exist. So use the curly braces {} when you expand variables. It’s safer overall.\nThere is an alternate method for variable expansion which we will use when we call a sub-shell - a shell within a shell, much like in our xargs command above. We need to use parentheses () to expand them within the sub-shell, but not the top-shell. We’ll use this when we process multiple files within a single worker.\n\n7.6.1 basename can be very handy when on workers\nIf we are processing a bunch of files on a worker, we need a way to get the bare filename from a dxfuse path. We will take advantage of this when we run process multiple files on the worker.\nFor example:\nbasename /mnt/project/worker_scripts/srun-script.sh\nThis will return:\nsrun-script.sh\nWhich can be really handy when we name our outputs. This command is so handy it is used in WDL.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Shell Scripting Basics</span>"
    ]
  },
  {
    "objectID": "scripting-basics.html#quoting-and-escaping-filenames-in-bash",
    "href": "scripting-basics.html#quoting-and-escaping-filenames-in-bash",
    "title": "7  Shell Scripting Basics",
    "section": "7.7 Quoting and Escaping Filenames in Bash",
    "text": "7.7 Quoting and Escaping Filenames in Bash\nOne point of confusion is when do you quote things in Bash? When do you use single quotes (') versus double-quotes (\")? When do you use \\ to escape characters?\nLet’s talk about some quoting rules in Bash. I’ve tried to make things as simplified and generalized as possible, rather than stating all of the rules for each quote.\n\nIf you have spaces in a filename, use double quotes (\"chr 1.bam\")\nIf you have a single quote in the filename, use double quotes to wrap it (\"ted's file.bam\")\nOnly escape characters when necessary - if you can solve a problem with quotes, use them\nIf you need to preserve an escaped character, use single quotes\n\nLet’s go over each of these with an example.\n\n7.7.1 If you have spaces in a filename, use double quotes (Most common)\nFor example, if your filename is chr 1 file.bam, then use double quotes in your argument\nsamtools view -c \"chr 1 file.bam\"\n\n\n7.7.2 If you have a single quote in the name, use double quotes to wrap it (less common)\nSay you have a file called ted's new file.bam. This can be a problem when you are calling it, especially because of the single quote.\nIn this case, you can do this:\nsamtools view -c \"ted's new file.bam\"\n\n\n7.7.3 Only escape characters when necessary (less common)\nThere are a number of special characters (such as Tab, and Newline) that can be specified as escape characters. In double quotes, characters such as $ are signals to Bash to expand or evaluate code.\nSay that someone had a $ in their file name such as Thi$file is money.bam\nHow do we refer to it? We can escape the character with a backslash \\:\nsamtools view -c \"Thi\\$file is money.bam\"\nThe backslash is a clue to Bash that we don’t want variable expansion in this case. Without it, bash would look for a variable called $file.\n\n\n7.7.4 If you need to preserve an escaped character, use single quotes (least common)\nThis is rarely used, but if you need to keep an escaped character in your filename, you can use single quotes. Say we have a filename called Thi\\$file.bam and you need that backslash in the file name (btw, please don’t do this), you can use single quotes to preserve that backslash:\nsamtools view -c 'Thi\\$file.bam'\nAgain, hopefully you won’t need this.\n\n\n7.7.5 For More Info\nhttps://www.grymoire.com/Unix/Quote.html#uh-3\n\n\n\n\n\n\nWhat about backticks?\n\n\n\nBackticks (`) are an old way to do command evaluation in Bash. For example, if we run the following on the command-line:\necho \"there are `ls -l | wc -l` files in this directory\"\nWill produce:\nthere are       36 files in this directory\nTheir use is deprecated, so you should be using $() in your command evaluations instead:\necho \"there are $(ls -l | wc -l) files in this directory\"\n\n\n\n\n\n\n\n\nWhat about X use case?\n\n\n\nThere are a lot of rules for Bash variable expansion and quoting that I don’t cover here. I try to show you a way to do things that work in multiple situations on the cluster.\nThat’s why I focus on double quotes for filenames and ${} for variable expansion in general. They will work whether your Bash script is on the command line or in an App, or in WDL.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Shell Scripting Basics</span>"
    ]
  },
  {
    "objectID": "scripting-basics.html#what-you-learned-in-this-chapter",
    "href": "scripting-basics.html#what-you-learned-in-this-chapter",
    "title": "7  Shell Scripting Basics",
    "section": "7.8 What you learned in this chapter",
    "text": "7.8 What you learned in this chapter\nWhew, this was a whirlwind tour. Keep this chapter in mind when you’re working on a cluster - the bash programming patterns will serve you well. We’ll refer to these patterns a lot when we get to doing more bioinformatics tasks on the cluster.\n\nSetting up bash scripts with positional arguments\nIterating over a list of files using xargs\nHow to use bash variables and variable expansions",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Shell Scripting Basics</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html",
    "href": "interactive_shell.html",
    "title": "7  Interactively Testing Scripts in Nodes",
    "section": "",
    "text": "7.1 Visual Table of Contents\nflowchart TD\n   A[Open Shell on Worker] --&gt; B\n   B[Test Scripts] --&gt; F\n   F[Exit Worker]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactively Testing Scripts in Nodes</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#learning-objectives",
    "href": "interactive_shell.html#learning-objectives",
    "title": "7  Interactively Testing Scripts in Nodes",
    "section": "7.2 Learning Objectives",
    "text": "7.2 Learning Objectives\nAfter reading this, you will be able to:\n\nOpen an interactive shell on a SLURM worker node using salloc or grabnode\nTest your code on a SLURM worker\nExit the shell of a remote worker\nUtilize screen to keep a shell open and running on a remote worker.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactively Testing Scripts in Nodes</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#grabbing-an-interactive-shell",
    "href": "interactive_shell.html#grabbing-an-interactive-shell",
    "title": "6  Testing Scripts Using Interactive Shell",
    "section": "6.2 Grabbing an interactive shell",
    "text": "6.2 Grabbing an interactive shell\nWhen you’re testing code that’s going to run on a worker node, you need to be aware of what the worker node sees.\nIt’s also important in estimating how long our tasks are going to run since we can test how long a task runs for a representative dataset.\nConfusingly, depending on On a SLURM system, the way to open interactive shells on a node has changed. Check your version first:\nsrun --version\nIf the version is past 20.11, we can open an interactive shell on a worker with salloc.\n\n\n\n\n\n\nFor FH Users: grabnode\n\n\n\nOn the FH system, we can use a command called grabnode, which will let us request a node. It will ask us for our requirements (numbers of cores, memory, etc.) for our node.\n#| eval: false\ntladera2@rhino01:~$ grabnode\ngrabnode will then ask us for what kind of instance we want, in terms of CPUs, Memory, and GPUs. Here, I’m grabbing a node with 8 cores, 8 Gb of memory, using it for 1 day, and no GPU.\nHow many CPUs/cores would you like to grab on the node? [1-36] 8\nHow much memory (GB) would you like to grab? [160] 8\nPlease enter the max number of days you would like to grab this node: [1-7] 1\nDo you need a GPU ? [y/N]n\n\nYou have requested 8 CPUs on this node/server for 1 days or until you type exit.\n\nWarning: If you exit this shell before your jobs are finished, your jobs\non this node/server will be terminated. Please use sbatch for larger jobs.\n\nShared PI folders can be found in: /fh/fast, /fh/scratch and /fh/secure.\n\nRequesting Queue: campus-new cores: 8 memory: 8 gpu: NONE\nsrun: job 40898906 queued and waiting for resources\nAfter a little bit, you’ll arrive at a new prompt:\n(base) tladera2@gizmok164:~$\nIf you’re doing interactive analysis that is going to span over a few days, I recommend that you use screen or tmux.\n\n\n\n\n\n\n\n\nRemember hostname\n\n\n\nWhen you are doing interactive analysis, it is easy to forget in which node you’re working in. Just as a quick check, I use hostname to remind myself whether I’m in rhino, gizmo, or within an apptainer container.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Testing Scripts in Containers Interactively</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#containers",
    "href": "interactive_shell.html#containers",
    "title": "6  Testing Scripts Using Interactive Shell",
    "section": "6.3 Containers",
    "text": "6.3 Containers\n\n6.3.1 Pulling a Docker Container\nLet’s pull a docker container from the Docker registry. Note we have to specify docker:// when we pull the container.\napptainer pull docker://biocontainers/samtools:v1.9-4-deb_cv1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Testing Scripts in Containers Interactively</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#opening-a-shell-in-a-docker-container",
    "href": "interactive_shell.html#opening-a-shell-in-a-docker-container",
    "title": "6  Testing Scripts Using Interactive Shell",
    "section": "6.4 Opening a Shell in a Docker Container",
    "text": "6.4 Opening a Shell in a Docker Container\nWhen you’re getting started, opening a shell using Apptainer can help you test out things like filepaths and how they’re accessed in the container. It’s hard to get an intuition for how this works until you can see the limited view from the container.\nYou can open an Apptainer shell in a container using apptainer shell. Remember to use docker:// before the container name. For example:\n#| eval:false\napptainer shell docker://biocontainers/samtools:v1.9-4-deb_cv1\nThis will open a Bash shell in the container. Once you’re in the container, you can test code, especially seeing whether your files can be seen by the container (see Section 6.4.1).\n\n6.4.1 Testing out bind paths in containers\nOne thing to keep in mind is that every container has its own filesystem. One of the hardest things to wrap your head around for containers is how their filesystems work, and how to access files that are outside of the container filesystem. We’ll call any filesystems outside of the container external filesystems to make the discussion a little easier.\nBy default, the containers have access to your current working directory. We could make this where our scripts live (such as /home/tladera2/), but because our data is elsewhere, we’ll need to specify that location (/fh/fast/mylab/) as well.\nThe main mechanism we have in Apptainer to access the external filesystem are bind paths. Much like mounting a drive, we can bind directories from the external filesystem using these bind points.\n\n\n\n\n\nflowchart LR\n    direction LR\n        A[Container Filesystem\\n/mydata/]--write--&gt;C(\"--bind /fh/fast/mydata/:/mydata/\")\n        C --read--&gt; A\n        B[\"External Directory\\n/fh/fast/mydata/\"] --read--&gt; C\n        C --write--&gt; B\n\n\n\n\n\n\nI think of bind paths as “tunnels” that give access to particular folders in the external filesystem. Once the tunnel is open, we can access data files, process them, and save them using the bind path.\nSay my data lives in /fh/fast/mydata/. Then I can specify a bind point in my apptainer shell and apptainer run commands.\nWe can do this with the --bind option:\n#| eval:false\napptainer shell --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1\nNote that the bind syntax doesn’t have the trailing slash. That is, note that it is:\n--bind /fh/fast/mydata: ....\nRather than\n--bind /fh/fast/mydata/: ....\nNow our /fh/fast/mydata/ folder will be available as /mydata/ in my container. We can read and write files to this bind point.\n\n\n\n\n\n\nWDL makes this way easier\n\n\n\nA major point of failure with apptainer scripting is when our scripts aren’t using the right bind points.\nThis is one reason we recommend writing WDL Workflows and a workflow engine (such as Cromwell) to run your workflows, since you don’t have to worry about them, because they are handled by the workflow engine.\n\n\n\n\n6.4.2 Testing in the Apptainer Shell\nOk, now we have a bind point, so now we can test our script in the shell. For example, we can see if we are invoking samtools in the correct way and that our bind points work.\n#| eval:false\nsamtools view -c /mydata/my_bam_file.bam &gt; /mydata/bam_counts.txt\nTrying out scripts in the container is the best way to understand what the container can and can’t see.\n\n\n6.4.3 Exiting the container when you’re done\nYou can exit, like any shell you open. You should be out of the container. Confirm by using hostname to make sure you’re out of the container.\n\n\n6.4.4 More Info\n\nCarpentries Section on Apptainer Paths - this is an excellent resource if you want to dive deeper into undestanding container filesystems and bind points.\nMore about bind paths and other options.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Testing Scripts in Containers Interactively</span>"
    ]
  },
  {
    "objectID": "r-commandline.html",
    "href": "r-commandline.html",
    "title": "7  Running a R script on the command line",
    "section": "",
    "text": "7.1 The Basic Process",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Running a R script on the command line</span>"
    ]
  },
  {
    "objectID": "r-commandline.html#the-basic-process",
    "href": "r-commandline.html#the-basic-process",
    "title": "7  Running a R script on the command line",
    "section": "",
    "text": "Specify named arguments for your R script.\nWrap running Rscript in a bash file, passing on arguments to the R Script\nUse bash to execute your bash script, with appropriate arguments.\n\n\n\n\n\n\n\nFor FH Users\n\n\n\nIn your bash script you’ll need to load the appropriate environment module (usually at least fhR):\nmodule load fhR/4.3.3-foss-2023b\nYou can also run things from the Bioconductor / Rocker containers using Apptainer (?sec-fh-apptainer).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Running a R script on the command line</span>"
    ]
  },
  {
    "objectID": "r-commandline.html#using-rscript-on-the-command-line",
    "href": "r-commandline.html#using-rscript-on-the-command-line",
    "title": "7  Running a R script on the command line",
    "section": "7.2 Using Rscript on the command-line",
    "text": "7.2 Using Rscript on the command-line\nLet’s talk about wrapping R scripts in a Bash script. This might seem like an extra layer of redundancy, but remember that we need to specify our software environment before we run something, so our bash script lets us do that.\nOur main executable for running R on the command-line is Rscript.\nWhen we run R on the command line, it will look something like this:\nRscript process_data.R --input_file=my_genome_file.vcf\nNote that you can have named inputs when you run on the command line,",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Running a R script on the command line</span>"
    ]
  },
  {
    "objectID": "r-commandline.html#wrapping-it-up-in-a-bash-script",
    "href": "r-commandline.html#wrapping-it-up-in-a-bash-script",
    "title": "7  Running a R script on the command line",
    "section": "7.3 Wrapping it up in a bash script",
    "text": "7.3 Wrapping it up in a bash script\nSay you have an R Script you need to run on the command line. In our bash script, we can do the following:\n\n\n\nscripting-basics/wrap_r_script.sh\n\n#!/bin/bash\nRscript process_data.R input_file=\"${1}\"\n\n\nThis calls Rscript, which is the command line executable, to run our R script. Note that we have a named argument called input_file and it is done differently than in Bash - how do we use this in our R Script?\n\n7.3.1 Using Named Arguments in an R script\nWe can pass arguments from our bash script to our R script by using commandArgs() - this will populate a list of named arguments (such as CSVFILE) that are passed into the R Script. We assign the output of commandArgs() into the args object.\nWe refer to our CSVFILE argument as args$CSVFILE in our script.\n\n\n\nscripting-basics/r_script.R\n\nlibrary(tidyverse)\n\nargs &lt;- commandArgs()\n# Use arg$CSVFILE in read.csv\ncsv_file &lt;- read.csv(file=args$input_file)\n\n# Do some work with csv_file\ncsv_filtered &lt;- csv_file |&gt; dplyr::filter()\n\n# Write output\nwrite.csv(csv_filtered, file = paste0(args$CSVFILE, \"_filtered.csv\"))\n\n\n\n\n7.3.2 Running our R Script\nNow that we’ve set it up, we can run the R script from the command line as follows:\n\nbash my_bash_script.sh my_csvfile.csv \n\nIn our bash script, my_bash_script.sh, we’re using positional argument (for simplicity) to specify our csvfile, and then passing the positional argument to named ones (CSVFILE) for my_r_script.R.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Running a R script on the command line</span>"
    ]
  },
  {
    "objectID": "r-commandline.html#quarto-documents",
    "href": "r-commandline.html#quarto-documents",
    "title": "7  Running a R script on the command line",
    "section": "7.4 Quarto Documents",
    "text": "7.4 Quarto Documents\nQuarto is the next generation of RMarkdown and supports a number of output formats.\nYou might want to apply a workflow that you’ve built in a quarto document.\nThe main difference is that you’d use quarto run rather than Rscript run.\n\n#!/bin/bash\nquarto run my_quarto_doc.qmd",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Running a R script on the command line</span>"
    ]
  },
  {
    "objectID": "r-commandline.html#sec-apptainer",
    "href": "r-commandline.html#sec-apptainer",
    "title": "7  Running a R script on the command line",
    "section": "7.5 Apptainer",
    "text": "7.5 Apptainer\nApptainer (previous Singularity) is a secure way to run Docker containers on a HPC system. The commands are very similar to Docker, but aren’t.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Running a R script on the command line</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#grabbing-an-interactive-shell-on-a-worker",
    "href": "interactive_shell.html#grabbing-an-interactive-shell-on-a-worker",
    "title": "7  Interactively Testing Scripts in Nodes",
    "section": "7.3 Grabbing an interactive shell on a worker",
    "text": "7.3 Grabbing an interactive shell on a worker\nWhen you’re testing code that’s going to run on a worker node, you need to be aware of what the worker node sees.\nIt’s also important in estimating how long our tasks are going to run since we can test how long a task runs for a representative dataset.\nOn a SLURM system, the way to open interactive shells on a node has changed. Check your version first:\nsrun --version\nIf you’re on a version before 20.11, you can use srun -i --pty bash to open an interactive terminal on a worker:\nsrun -i --pty bash\nIf the version is past 20.11, we can open an interactive shell on a worker with salloc.\nsalloc bash\n\n\n\n\n\n\nFor FH Users: grabnode\n\n\n\nOn the FH system, we can use a command called grabnode, which will let us request a node. It will ask us for our requirements (numbers of cores, memory, etc.) for our node.\ntladera2@rhino01:~$ grabnode\ngrabnode will then ask us for what kind of instance we want, in terms of CPUs, Memory, and GPUs. Here, I’m grabbing a node with 8 cores, 8 Gb of memory, using it for 1 day, and no GPU.\nHow many CPUs/cores would you like to grab on the node? [1-36] 8\nHow much memory (GB) would you like to grab? [160] 8\nPlease enter the max number of days you would like to grab this node: [1-7] 1\nDo you need a GPU ? [y/N]n\n\nYou have requested 8 CPUs on this node/server for 1 days or until you type exit.\n\nWarning: If you exit this shell before your jobs are finished, your jobs\non this node/server will be terminated. Please use sbatch for larger jobs.\n\nShared PI folders can be found in: /fh/fast, /fh/scratch and /fh/secure.\n\nRequesting Queue: campus-new cores: 8 memory: 8 gpu: NONE\nsrun: job 40898906 queued and waiting for resources\nAfter a little bit, you’ll arrive at a new prompt:\n(base) tladera2@gizmok164:~$\nIf you’re doing interactive analysis that is going to span over a few days, I recommend that you use screen or tmux.\n\n\n\n\n\n\n\n\nRemember hostname\n\n\n\nWhen you are doing interactive analysis, it is easy to forget in which node you’re working in. Just as a quick check, I use hostname to remind myself whether I’m in rhino, gizmo, or within an apptainer container.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactively Testing Scripts in Nodes</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#opening-shells-in-containers-for-testing",
    "href": "interactive_shell.html#opening-shells-in-containers-for-testing",
    "title": "6  Testing Scripts in Containers Interactively",
    "section": "6.4 Opening Shells in Containers for Testing",
    "text": "6.4 Opening Shells in Containers for Testing\nIn this section, we talk about testing scripts in a container using apptainer. We use apptainer (formerly Singularity) in order to run Docker containers on a shared HPC system. This is because Docker itself requires root-level privileges, which is not secure on shared systems.\nIn order to do our testing, we’ll first pull the Docker container, map our bind point (so our container can access files outside of its file system), and then run scripts in the container.\nEven if you aren’t going to frequently use Apptainer in your work, I recommend trying an interactive shell in a container at least once or twice to learn about the container filesystem and how you connect it to the external filesystem.\n\n6.4.1 Pulling a Docker Container\nLet’s pull a docker container from the Docker registry. Note we have to specify docker:// when we pull the container, because Apptainer has its own internal format called SIF.\napptainer pull docker://biocontainers/samtools:v1.9-4-deb_cv1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Testing Scripts in Containers Interactively</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#opening-a-shell-in-a-container-with-apptainer-shell",
    "href": "interactive_shell.html#opening-a-shell-in-a-container-with-apptainer-shell",
    "title": "6  Testing Scripts in Containers Interactively",
    "section": "6.5 Opening a Shell in a Container with apptainer shell",
    "text": "6.5 Opening a Shell in a Container with apptainer shell\nWhen you’re getting started, opening a shell using Apptainer can help you test out things like filepaths and how they’re accessed in the container. It’s hard to get an intuition for how file I/O works with containers until you can see the limited view from the container.\nBy default, apptainers can see your current directory and navigate to the files in it.\nYou can open an Apptainer shell in a container using apptainer shell. Remember to use docker:// before the container name. For example:\n#!/bin/bash\nmodule load Apptainer/1.1.6\napptainer shell docker://biocontainers/samtools:v1.9-4-deb_cv1\nThis will open a Bash shell in the container. Once you’re in the container, you can test code, especially seeing whether your files can be seen by the container (see Section 6.5.1). 90% of the issues with using Docker containers has to do with bind paths, so we’ll talk about that next.\nOnce you’re in the shell, you can take a look at where samtools is installed:\nwhich samtools\nNote that the container filesystem is isolated, and we need to explicitly build connections to it (called bind paths) to get files in and out. We’ll talk more about this in the next section.\nOnce we’re done testing scripts in our containers, we can exit the shell and get back into the node.\nexit\n\n\n\n\n\n\nOpening a Shell in a Docker Container\n\n\n\nFor the most part, due to security reasons, we don’t use docker on HPC systems. However, if you have admin level access (for example, on your own laptop), you can open up an interactive shell with docker run:\ndocker run -it biocontainers/samtools:v1.9-4-deb_cv1 /bin/bash\n\n\n\n6.5.1 Testing out bind paths in containers\nOne thing to keep in mind is that every container has its own filesystem. One of the hardest things to wrap your head around for containers is how their filesystems work, and how to access files that are outside of the container filesystem. We’ll call any filesystems outside of the container external filesystems to make the discussion a little easier.\nBy default, the containers have access to your current working directory. We could make this where our scripts live (such as /home/tladera2/), but because our data is elsewhere, we’ll need to specify that location (/fh/fast/mylab/) as well.\nThe main mechanism we have in Apptainer to access the external filesystem are bind paths. Much like mounting a drive, we can bind directories from the external filesystem using these bind points.\n\n\n\n\n\nflowchart LR\n   A[\"Container Filesystem\\n/mydata/\"]--write--&gt;C(\"--bind /fh/fast/mydata/:/mydata/\")\n   C --read--&gt; A\n   B[\"External Directory\\n/fh/fast/mydata/\"] \n   B --read--&gt; C\n   C --write--&gt; B\n\n\n\n\n\n\nI think of bind paths as “tunnels” that give access to particular folders in the external filesystem. Once the tunnel is open, we can access data files, process them, and save them using the bind path.\nSay my data lives in /fh/fast/mydata/. Then I can specify a bind point in my apptainer shell and apptainer run commands.\nWe can do this with the --bind option:\napptainer shell --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1\nNote that the bind syntax doesn’t have the trailing slash (/). That is, note that it is:\n--bind /fh/fast/mydata: ....\nRather than\n--bind /fh/fast/mydata/: ....\nNow our /fh/fast/mydata/ folder will be available as /mydata/ in my container. We can read and write files to this bind point. For example, I’d refer to the .bam file /fh/fast/mydata/my_bam_file.bam as:\nsamtools view -c /mydata/my_bam_file.bam\n\n\n\n\n\n\nWDL makes this way easier\n\n\n\nA major point of failure with Apptainer scripting is when our scripts aren’t using the right bind paths.\nThis is one reason we recommend writing WDL Workflows and a workflow engine (such as Cromwell) to run your workflows. You don’t have to worry that your bind points are setup correctly, because they are handled by the workflow engine.\n\n\n\n\n6.5.2 Testing in the Apptainer Shell\nOk, now we have a bind point, so now we can test our script in the shell. For example, we can see if we are invoking samtools in the correct way and that our bind points work.\nsamtools view -c /mydata/my_bam_file.bam &gt; /mydata/bam_counts.txt\nAgain, trying out scripts in the container is the best way to understand what the container can and can’t see.\n\n\n6.5.3 Exiting the container when you’re done\nYou can exit, like any shell you open. You should be out of the container. Confirm by using hostname to make sure you’re out of the container.\n\n\n6.5.4 More Info\n\nCarpentries Section on Apptainer Paths - this is an excellent resource if you want to dive deeper into undestanding container filesystems and bind points.\nMore about bind paths and other options.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Testing Scripts in Containers Interactively</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#what-next",
    "href": "interactive_shell.html#what-next",
    "title": "7  Interactively Testing Scripts in Nodes",
    "section": "7.7 What Next?",
    "text": "7.7 What Next?\nYou’ve learned how to open interactive shells in a remote node. There are a number of ways to go from here:\n\nTry running your script using srun\nTry running your script in a container (Chapter 8)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactively Testing Scripts in Nodes</span>"
    ]
  },
  {
    "objectID": "hpc-basics.html#requesting-machines",
    "href": "hpc-basics.html#requesting-machines",
    "title": "2  HPC Basics",
    "section": "2.6 Requesting Machines",
    "text": "2.6 Requesting Machines\nHow do you request a set of machines on the HPC? There are multiple ways to do so:\n\nOpen an Interactive shell on a Node\nAs part of a job using srun or sbatch\nUsing Cromwell or Nextflow\n\nIn general, we recommend",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>HPC Basics</span>"
    ]
  },
  {
    "objectID": "running_containers.html",
    "href": "running_containers.html",
    "title": "10  Working with containers",
    "section": "",
    "text": "10.1 Visual Table of Contents\nflowchart TD\n   B[\"Open Shell in Container\\n(with Bindpaths)\"]\n   B --&gt; D[Test Scripts in container]\n   D --&gt; E[Exit Container]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with containers</span>"
    ]
  },
  {
    "objectID": "running_containers.html#opening-shells-in-containers-for-testing",
    "href": "running_containers.html#opening-shells-in-containers-for-testing",
    "title": "10  Working with containers",
    "section": "10.2 Opening Shells in Containers for Testing",
    "text": "10.2 Opening Shells in Containers for Testing\nIn this section, we talk about testing scripts in a container using apptainer. We use apptainer (formerly Singularity) in order to run Docker containers on a shared HPC system. This is because Docker itself requires root-level privileges, which is not secure on shared systems.\nIn order to do our testing, we’ll first pull the Docker container, map our bind point (so our container can access files outside of its file system), and then run scripts in the container.\nEven if you aren’t going to frequently use Apptainer in your work, I recommend trying an interactive shell in a container at least once or twice to learn about the container filesystem and conceptually understand how you connect it to the external filesystem.\n\n10.2.1 Pulling a Docker Container\nLet’s pull a docker container from the Docker registry. Note we have to specify docker:// when we pull the container, because Apptainer has its own internal format called SIF.\napptainer pull docker://biocontainers/samtools:v1.9-4-deb_cv1",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with containers</span>"
    ]
  },
  {
    "objectID": "running_containers.html#opening-a-shell-in-a-container-with-apptainer-shell",
    "href": "running_containers.html#opening-a-shell-in-a-container-with-apptainer-shell",
    "title": "10  Working with containers",
    "section": "10.3 Opening a Shell in a Container with apptainer shell",
    "text": "10.3 Opening a Shell in a Container with apptainer shell\nWhen you’re getting started, opening a shell using Apptainer can help you test out things like filepaths and how they’re accessed in the container. It’s hard to get an intuition for how file I/O works with containers until you can see the limited view from the container.\nBy default, apptainers can see your current directory and navigate to the files in it.\nYou can open an Apptainer shell in a container using apptainer shell. Remember to use docker:// before the container name. For example:\nmodule load Apptainer/1.1.6\napptainer shell docker://biocontainers/samtools:v1.9-4-deb_cv1\nThis will load the apptainer module, and then open a Bash shell in the container using apptainer shell. Once you’re in the container, you can test code, especially seeing whether your files can be seen by the container (see Section 10.4). 90% of the issues with using Docker containers has to do with bind paths, so we’ll talk about that next.\nOnce you’re in the shell, you can take a look at where samtools is installed:\nwhich samtools\nNote that the container filesystem is isolated, and we need to explicitly build connections to it (called bind paths) to get files in and out. We’ll talk more about this in the next section.\nOnce we’re done testing scripts in our containers, we can exit the shell and get back into the node.\nexit\n\n\n\n\n\n\nOpening a Shell in a Docker Container with Docker\n\n\n\nFor the most part, due to security reasons, we don’t use docker on HPC systems. In short, the docker group essentially has root-level access to the machine, and it’s not a good for security on a shared resource like an HPC. However, if you have admin level access (for example, on your own laptop), you can open up an interactive shell with docker run:\ndocker run -it biocontainers/samtools:v1.9-4-deb_cv1 /bin/bash\nThis will open a bash shell much like apptainer shell. Note that volumes (the docker equivalent of bind paths) are specified differently in Docker compared to Apptainer.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with containers</span>"
    ]
  },
  {
    "objectID": "running_containers.html#sec-bindpaths",
    "href": "running_containers.html#sec-bindpaths",
    "title": "10  Working with containers",
    "section": "10.4 Testing out bind paths in containers",
    "text": "10.4 Testing out bind paths in containers\nOne thing to keep in mind is that every container has its own filesystem. One of the hardest things to wrap your head around for containers is how their filesystems work, and how to access files that are outside of the container filesystem. We’ll call any filesystems outside of the container external filesystems to make the discussion a little easier.\nBy default, the containers have access to your current working directory. We could make this where our scripts live (such as /home/tladera2/), but because our data is elsewhere, we’ll need to specify that location (/fh/fast/mylab/) as well.\nThe main mechanism we have in Apptainer to access the external filesystem are bind paths. Much like mounting a drive, we can bind directories from the external filesystem using these bind points.\n\n\n\n\n\nflowchart LR\n   B[\"External Directory\\n/fh/fast/mydata/\"] \n   B --read--&gt; C\n   C --write--&gt; B\n   A[\"Container Filesystem\\n/mydata/\"]--write--&gt;C(\"--bind /fh/fast/mydata/:/mydata/\")\n   C --read--&gt; A\n\n\n\n\n\n\nI think of bind paths as “tunnels” that give access to particular folders in the external filesystem. Once the tunnel is open, we can access data files, process them, and save them using the bind path.\nSay my data lives in /fh/fast/mydata/. Then I can specify a bind point in my apptainer shell and apptainer run commands.\nWe can do this with the --bind option:\napptainer shell --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1\nNote that the bind syntax doesn’t have the trailing slash (/). That is, note that it is:\n--bind /fh/fast/mydata: ....\nRather than\n--bind /fh/fast/mydata/: ....\nNow our /fh/fast/mydata/ folder will be available as /mydata/ in my container. We can read and write files to this bind point. For example, I’d refer to the .bam file /fh/fast/mydata/my_bam_file.bam as:\nsamtools view -c /mydata/my_bam_file.bam\n\n\n\n\n\n\nWDL makes this way easier\n\n\n\nA major point of failure with Apptainer scripting is when our scripts aren’t using the right bind paths.\nThis is one reason we recommend writing WDL Workflows and a workflow engine (such as Cromwell) to run your workflows. You don’t have to worry that your bind points are setup correctly, because they are handled by the workflow engine.\n\n\n\n10.4.1 Testing in the Apptainer Shell\nOk, now we have a bind point, so now we can test our script in the shell. For example, we can see if we are invoking samtools in the correct way and that our bind points work.\nsamtools view -c /mydata/my_bam_file.bam &gt; /mydata/bam_counts.txt\nAgain, trying out scripts in the container is the best way to understand what the container can and can’t see.\n\n\n10.4.2 Exiting the container when you’re done\nYou can exit, like any shell you open. You should be out of the container. Confirm by using hostname to make sure you’re out of the container.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with containers</span>"
    ]
  },
  {
    "objectID": "running_containers.html#more-info",
    "href": "running_containers.html#more-info",
    "title": "10  Working with containers",
    "section": "10.8 More Info",
    "text": "10.8 More Info\n\nCarpentries Section on Apptainer Paths - this is an excellent resource if you want to dive deeper into undestanding container filesystems and bind points.\nApptainer Documentation on Bind Paths. There are a lot of good examples here on how to set up bind paths.\nMore about bind paths and other options.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with containers</span>"
    ]
  },
  {
    "objectID": "running_containers.html#what-next",
    "href": "running_containers.html#what-next",
    "title": "10  Working with containers",
    "section": "10.9 What Next?",
    "text": "10.9 What Next?\nYou’ve learned how to open interactive shells in both a remote node and in a container. There are a number of ways to go from here:\n\nIntegrate containers into your scripts\nCustomize your containers using Dockerfiles",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with containers</span>"
    ]
  },
  {
    "objectID": "running_containers.html#entrypoints",
    "href": "running_containers.html#entrypoints",
    "title": "10  Working with containers",
    "section": "10.7 Entrypoints",
    "text": "10.7 Entrypoints\nYou can think of an entrypoint as a way of automatically starting something up in your container when you run it. For example, if you have a web stack container, one of the things you want to start is a web server when you start running it.\nThe main reason to be aware of entrypoints is when they exist in a Dockerfile. It is important to know what is started when you start running a container.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with containers</span>"
    ]
  },
  {
    "objectID": "working-with-wdl.html",
    "href": "working-with-wdl.html",
    "title": "9  Working with WDL",
    "section": "",
    "text": "9.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with WDL</span>"
    ]
  },
  {
    "objectID": "working-with-wdl.html#learning-objectives",
    "href": "working-with-wdl.html#learning-objectives",
    "title": "9  Working with WDL",
    "section": "",
    "text": "Explain the basic architecture of a WDL file\nExplain the role of a task in WDL\nUtilize Cromwell to execute a WDL script on one file\nUttilze Cromwell to batch execute a WDL script on multiple files",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with WDL</span>"
    ]
  },
  {
    "objectID": "working-with-wdl.html#architecture-of-a-wdl-file",
    "href": "working-with-wdl.html#architecture-of-a-wdl-file",
    "title": "9  Working with WDL",
    "section": "9.2 Architecture of a WDL file",
    "text": "9.2 Architecture of a WDL file\nThe best way to read WDL files is to read them top down. We’ll focus on the basic sections of a WDL file before we see how they work together.\nThe code below is from the WILDs WDL Repo.\nworkflow SRA_STAR2Pass {\n  input { \n    Array[String] sra_id_list\n    RefGenome ref_genome\n  }\n\n  scatter ( id in sra_id_list ){\n    call fastqdump {\n        ...\n    }\n\n    call STARalignTwoPass {\n        ...\n    }\n  } # End scatter \n\n  # Outputs that will be retained when execution is complete\n  output {\n    ...\n  }\n\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with WDL</span>"
    ]
  },
  {
    "objectID": "working-with-wdl.html#anatomy-of-a-task",
    "href": "working-with-wdl.html#anatomy-of-a-task",
    "title": "9  Working with WDL",
    "section": "9.3 Anatomy of a Task",
    "text": "9.3 Anatomy of a Task\ntask fastqdump {\n  input {\n    String sra_id\n    Int ncpu = 12\n  }\n\n  command &lt;&lt;&lt;\n    set -eo pipefail\n    # check if paired ended\n    numLines=$(fastq-dump -X 1 -Z --split-spot \"~{sra_id}\" | wc -l)\n    paired_end=\"false\"\n    if [ $numLines -eq 8 ]; then\n      paired_end=\"true\"\n    fi\n    # perform fastqdump\n    if [ $paired_end == 'true' ]; then\n      echo true &gt; paired_file\n      parallel-fastq-dump \\\n        --sra-id ~{sra_id} \\\n        --threads ~{ncpu} \\\n        --outdir ./ \\\n        --split-files \\\n        --gzip\n    else\n      touch paired_file\n      parallel-fastq-dump \\\n        --sra-id ~{sra_id} \\\n        --threads ~{ncpu} \\\n        --outdir ./ \\\n        --gzip\n    fi\n  &gt;&gt;&gt;\n\n  output {\n    File r1_end = \"~{sra_id}_1.fastq.gz\"\n    File r2_end = \"~{sra_id}_2.fastq.gz\"\n    String paired_end = read_string('paired_file')\n  }\n\n  runtime {\n    memory: 2 * ncpu + \" GB\"\n    docker: \"getwilds/pfastqdump:0.6.7\"\n    cpu: ncpu\n  }\n\n  parameter_meta {\n    ...\n  }\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with WDL</span>"
    ]
  },
  {
    "objectID": "working-with-wdl.html#resources",
    "href": "working-with-wdl.html#resources",
    "title": "9  Working with WDL",
    "section": "9.4 Resources",
    "text": "9.4 Resources\n\nDeveloping WDL Workflows is a full guide from the Data Science Lab (DaSL) showing you how to develop your own WDL Workflows and has a much more in detail section of WDL file architecture.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with WDL</span>"
    ]
  },
  {
    "objectID": "intro-unix.html#where-am-i-understanding-the-unix-filesystem",
    "href": "intro-unix.html#where-am-i-understanding-the-unix-filesystem",
    "title": "3  Everything about Unix/Linux they didn’t teach you",
    "section": "3.3 Where Am I? Understanding the Unix Filesystem",
    "text": "3.3 Where Am I? Understanding the Unix Filesystem\nOne of the most confusing things about working on HPC is that sometimes you have a shell open on the head node, but oftentimes, you are on a worker node.\nYour totem for telling which node you’re in is hostname, which will give you the host name of the machine you’re on.\nFor example, if I used grabnode to grab a gizmo node for interactive work, I can check which node I’m in by using:\nhostname\ngizmok164\nIf you’re confused about which node you’re in, remember hostname. It will save you from making mistakes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "filesystems.html",
    "href": "filesystems.html",
    "title": "5  Navigating the Unix Filesystem",
    "section": "",
    "text": "5.1 Learning Objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#where-am-i-understanding-the-unix-filesystem",
    "href": "filesystems.html#where-am-i-understanding-the-unix-filesystem",
    "title": "4  Navigating the Unix Filesystem",
    "section": "",
    "text": "hostname\ngizmok164",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#navigating-the-unixlinux-filesystem",
    "href": "filesystems.html#navigating-the-unixlinux-filesystem",
    "title": "4  Navigating the Unix Filesystem",
    "section": "4.3 Navigating the Unix/Linux filesystem",
    "text": "4.3 Navigating the Unix/Linux filesystem\nWe’ll start out our Unix/Linux journey talking about the filesystem, which has some quirks we need to be aware of.\n\n\n\n\n\n\nFH users: the main filesystems\n\n\n\nWhen working on the HPC, there are three filesystems you should consider:\n\n/home/ - The home filesystem. Your scripts can live here\n/fh/fast/ (also known as fast) - Research storage. Raw files and processed results should live here.\n/fh/temp/ (also known as temp) - The temporary filesystem. This filesystem is faster to access for gizmo nodes on the cluster, so files can be copied to for computation. The output files you generate should be moved back into an appropriate folder on /fh/fast/. Note that files on /fh/temp/ will be deleted after 30 days.\n\nBelow is a a diagram with one way to work with these multiple filesystems.\n\nWe transfer the raw files to be processed from /fh/fast/ to our directory /fh/temp/. For example, a set of .bam files.\nWe run our scripts from /home/, on the raw files in /fh/temp/ and produce results in /fh/temp/\nWe transfer our results from /fh/temp/ to /fh/fast/\n\n\n\n\n\n\ngraph LR\n    A[\"Home Directory\\n/home/tladera2/\\nScripts\"] --\"2. run scripts\"--&gt; C\n    B[\"Fast\\n/fh/fast/tladera2\\nResearch Data\"] --\"1. transfer\\nraw files\"--&gt; C\n    C[\"Temp\\n/fh/temp/tladera2\"] --\"3. transfer\\nresults\"--&gt; B\n\n\n\n\n\n\nYour main mechanism for getting files to and from fast and scratch is Motuz, which is a GUI based file transfer utility. One of the advantages of Motuz is that it supports file resuming in case of disconnection, avoiding having to reupload the whole batch of files over again.\nMotuz also supports file transfers between other filesystems at FH, including the FH Amazon S3 bucket. Highly recommended.\n\n\n\n4.3.1 When in doubt: pwd\nThe pwd command (short for present working directory) will let you know your current location in the filesystem. Knowing your current directory is critical when using relative file paths.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#projectfolder-based-workflows",
    "href": "filesystems.html#projectfolder-based-workflows",
    "title": "4  Navigating the Unix Filesystem",
    "section": "4.3 Project/folder based workflows",
    "text": "4.3 Project/folder based workflows\nOn a particular machine, using absolute paths is safe. However, you do this at the cost of portability - code that you write on one machine may not run on another.\nIf you ever anticipate doing the analysis on a separate machine, using project structures with relative paths is the safest.\nFor example, here’s one example of putting everything into a single folder:\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nIn the above example, our project is named my_project, and there are three folders inside it: data/, results/, and scripts/. Our main script for running is my_project/run_workflow.sh. Because this script is in the root folder, we can refer to the data/ folder to process files:\n./scripts/run_bowtie.sh data/*.fa.gz results/\nWhen we run run_workflow.sh, it will execute run_bowtie.sh on all of the files in data/, and save them in results/, resulting in the following updated structure.\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n│   ├── chr1.bam\n│   ├── chr2.bam\n│   └── chr3.bam\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nYou may have seen relative paths such as ../another_directory/ - the .. means to go up a directory in the file hierarchy, and then look in that directory for the another_directory/ directory. I try to avoid using relative paths like these.\nIn general for portability and reproducibility, you will want to use relative paths within a directory, and avoid using relative paths like ../../my_folder, where you are navigating up. In general, use relative paths to navigate down.\n\n4.3.1 Moving Things Around\nA lot of the time, we need to move files between shared filesystems. One filesystem might be good at storage and be backed up on a regular basis, while another filesystem might be better for temporary work on the cluster.\nYou might be familiar with mv, which lets you move files around in Unix. One thing to keep in mind when you’re mving things to a new folder that there is a difference between:\nmv log.txt my_folder   ## renames log.txt to my_folder\nand\nmv log.txt my_folder/  ## moves log.txt to be in my_folder\nThis is one thing that trips me up all the time.\nThis is one situation where using a GUI such as Motuz can be very helpful.\n\n\n4.3.2 Things I always forget: the difference between /home/mydir/ and home/mydir/\nSome things that trip me up all the time. The difference between\n/home/mydir/    #absolute path\nand\nhome/mydir/    #relative path\nThe first one is an absolute path, and the second is a relative path. Your clue is the leading / at the beginning of a path. If you’re getting file not found messages, check to make sure the path is the right format.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#learning-objectives",
    "href": "filesystems.html#learning-objectives",
    "title": "5  Navigating the Unix Filesystem",
    "section": "",
    "text": "Navigate the HPC filesystem using both absolute and relative paths\nIdentify the shared filesystems associated with your cluster\nDevelop Scripts that use multiple filesystems to process data\nExplain the benefits of a folder-based approach to organizing your analysis\nMove Files between filesystems and folders",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#visual-table-of-contents",
    "href": "filesystems.html#visual-table-of-contents",
    "title": "5  Navigating the Unix Filesystem",
    "section": "5.2 Visual Table of Contents",
    "text": "5.2 Visual Table of Contents\n\n\n\n\n\nflowchart TD\n    A[\"Main Filesystems on FH\"] --&gt; B\n    click A \"#sec-filesystems\"\n    B[\"Home Directories\"] --&gt; C\n    click B \"#sec-home\"\n    C[\"Absolute/Relative Paths\"] --&gt; D\n    click C \"#sec-paths\"\n    D[\"Project Based File Structures\"] --&gt; E\n    click D \"#sec-project\"\n    E[\"Moving Things Around\"]\n    click E \"#sec-moving\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#sec-home",
    "href": "filesystems.html#sec-home",
    "title": "5  Navigating the Unix Filesystem",
    "section": "5.4 Going /home: ~/",
    "text": "5.4 Going /home: ~/\nThere is one important file alias you should always remember: ~/ is shorthand for your own home directory.\nDepending on the linux distribution, this can be a different location. On the FH filesystem, when I use ~/, it maps to:\n/home/tladera2/\nThe home directory is also important because it is where your configuration files live, such as .bashrc (see Section 3.4.4).\nDepending on how you work, you may want to store your scripts and workflows in /home/. Some people prefer to keep their scripts, data, and results in a single folder. For more info, see Section 5.6.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#absolute-versus-relative-paths",
    "href": "filesystems.html#absolute-versus-relative-paths",
    "title": "4  Navigating the Unix Filesystem",
    "section": "4.5 Absolute versus relative paths",
    "text": "4.5 Absolute versus relative paths\nAbsolute paths contain all the information needed to find a file in a file system from the root / directory. For example, this would be an absolute path:\n/fh/fast/laderast/immuno_project/raw_data/tcr_data.fasta\nIn terms of folder structure, this is what this looks like:\n/\n├── fh\n│   └──fast\n│       └──laderast\n|            └──immuno_project\n│                 └──raw_data\n│                    └──chr2.fa.gz\nAbsolute paths always start with /, because that is the root directory, where all the top folders and files live.\nRelative paths break up an absolute path into two pieces of information: 1) your current directory and 2) the path relative to that directory. Relative paths are really helpful because things don’t break when you move your folder or files.\n/\n├── fh\n│   └──fast\n│       └── laderast\n|            └──immuno_project *\nIf my current path the directory /fh/fast/laderas_t/immuno_project/, then the relative path to that same file would be:\nraw_data/tcr_data.fasta\nNote that this relative path does not start with a /, because our current directory isn’t the root directory. Relative paths are incredibly useful when scripting in a reproducible manner, such as using project-based workflows to process files in a single folder.\n\n4.5.1 Keep Everything in a Folder\nWe need to talk about code and data organization. For the FH system, we have a /home/ directory, and if we have generated research data, a /fh/fast/ directory. If we want our scripts to live in /home/ and our data is in /fh/temp/, we’ll need to refer to each of these file locations.\nIdeally, we want to make the naming conventions of our code and our data as similar as possible.\nThe requirements of bioinformatics analysis usually require a huge amount of data, so storing data and scripts in a single directory is usually not recommended.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#sec-project",
    "href": "filesystems.html#sec-project",
    "title": "5  Navigating the Unix Filesystem",
    "section": "5.6 Project/folder based workflows",
    "text": "5.6 Project/folder based workflows\nOn a particular machine, using absolute paths is safe. However, you do this at the cost of portability - code that you write on one machine may not run on another.\nIf you ever anticipate doing the analysis on a separate machine, using project structures with relative paths is the safest. For example, you may want to move from the on-premise FH system to working with the data in AWS.\nHere’s one example of putting everything into a single folder:\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nIn the above example, our project is named my_project, and there are three folders inside it: data/, results/, and scripts/. Our main script for running is my_project/run_workflow.sh. Because this script is in the root folder, we can refer to the data/ folder to process files:\n./scripts/run_bowtie.sh data/*.fa.gz results/\nWhen we run run_workflow.sh, it will execute run_bowtie.sh on all of the files in data/, and save them in results/, resulting in the following updated structure.\nmy_project\n├── data\n│   ├── chr1.fa.gz\n│   ├── chr2.fa.gz\n│   └── chr3.fa.gz\n├── results\n│   ├── chr1.bam\n│   ├── chr2.bam\n│   └── chr3.bam\n├── run_workflow.sh\n└── scripts\n    └── run_bowtie.sh\nYou may have seen relative paths such as ../another_directory/ - the .. means to go up a directory in the file hierarchy, and then look in that directory for the another_directory/ directory. I try to avoid using relative paths like these.\nIn general for portability and reproducibility, you will want to use relative paths within a directory, and avoid using relative paths like ../../my_folder, where you are navigating up. In general, use relative paths to navigate down.\n\n\n\n\n\n\nWhy This is Important\n\n\n\nWhen you start executing scripts, it’s important to know where the results go. When you execute SAMtools on a file in /fh/temp/, for example, where does the output go?\nWorkflow Runners such as Cromwell and Nextflow will output into certain file structures by default. This can be changed, but knowing the default behavior is super helpful when you don’t specify an output directory.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#moving-things-around",
    "href": "filesystems.html#moving-things-around",
    "title": "4  Navigating the Unix Filesystem",
    "section": "4.7 Moving Things Around",
    "text": "4.7 Moving Things Around\nA lot of the time, we need to move files between shared filesystems. One filesystem might be good at storage and be backed up on a regular basis, while another filesystem might be better for temporary work on the cluster.\nYou might be familiar with mv, which lets you move files around in Unix. One thing to keep in mind when you’re mving things to a new folder that there is a difference between:\nmv log.txt my_folder   ## renames log.txt to my_folder\nand\nmv log.txt my_folder/  ## moves log.txt to be in my_folder\nThis is one thing that trips me up all the time.\nThis is one situation where using a GUI such as Motuz can be very helpful.\n\n4.7.1 Things I always forget: the difference between /home/mydir/ and home/mydir/\nSome things that trip me up all the time. The difference between\n/home/mydir/    #absolute path\nand\nhome/mydir/    #relative path\nThe first one is an absolute path, and the second is a relative path. Your clue is the leading / at the beginning of a path. If you’re getting file not found messages, check to make sure the path is the right format.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#sec-filesystems",
    "href": "filesystems.html#sec-filesystems",
    "title": "5  Navigating the Unix Filesystem",
    "section": "5.3 FH users: the main filesystems",
    "text": "5.3 FH users: the main filesystems\nWhen working on the Fred Hutch HPC, there are three main filesystems you should consider:\n\n/home/ - The home filesystem. Your scripts can live here. Also where your configuration files (such as .bashrc) live.\n/fh/fast/ (also known as fast) - Research storage. Raw files and processed results should live here.\n/fh/temp/ (also known as temp) - The temporary filesystem. This filesystem is faster to access for gizmo nodes on the cluster, so files can be copied to for computation. The output files you generate should be moved back into an appropriate folder on /fh/fast/. Note that files on /fh/temp/ will be deleted after 30 days.\n\n\n5.3.1 A Simplified Workflow: /fh/fast/ and /fh/temp/\nA simple approach is to have your scripts also live in your project folder in fast. Then, you can sync the project in /fh/fast/ over to /fh/temp/, run the scripts in /fh/temp/, and then sync the two folders again. You can do the file sync’ing in both directions with Motuz (Section 5.3.3), which has its own advantages.\nIf you want to go this route, you should think about using a Folder Based Workflow (Section 5.6), where everything lives in a folder.\nAnother thing to consider is to have a backup of the scripts that is either on your own machine or in GitHub. You can do this by using your .gitignore to exclude the data and results.\n\n\n\n\n\ngraph LR\n    A[\"Fast\\n/fh/fast/my_lab/project/\\nRaw Data & Scripts\"] --\"1. Sync Data\\n& scripts\"--&gt;B\n    B[\"Temp\\n/fh/temp/my_lab/project\\n2. Run Scripts here\"] --\"3. Sync\\nresults\"--&gt;A\n\n\n\n\n\n\n\n\n5.3.2 Another Approach\nBelow is a a diagram with another way to work with these multiple filesystems.\n\nWe transfer the raw files to be processed from /fh/fast/ to our directory /fh/temp/. For example, a set of .bam files.\nWe run our scripts from /home/, on the raw files in /fh/temp/ and produce results in /fh/temp/.\nWe transfer our results from /fh/temp/ to /fh/fast/.\n\n\n\n\n\n\ngraph LR\n    A[\"Home Directory\\n/home/tladera2/\\nScripts\"] --\"2. run scripts\"--&gt; C\n    B[\"Fast\\n/fh/fast/tladera2\\nResearch Data\"] --\"1. transfer\\nraw files\"--&gt; C\n    C[\"Temp\\n/fh/temp/tladera2\"] --\"3. transfer\\nresults\"--&gt; B\n\n\n\n\n\n\n\n\n5.3.3 Transferring Files between Filesystems\nYour main mechanism for getting files to and from fast and scratch is Motuz, which is a GUI based file transfer utility.\nOne of the advantages of Motuz is that it supports file resuming in case of disconnection, avoiding having to reupload the whole batch of files over again.\nMotuz also supports file transfers between other filesystems at FH, including the FH Amazon S3 bucket. Highly recommended.\n\n\n5.3.4 When in doubt: pwd\nThe pwd command (short for present working directory) will let you know your current location in the filesystem. Knowing your current directory is critical when using relative file paths.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#sec-moving",
    "href": "filesystems.html#sec-moving",
    "title": "5  Navigating the Unix Filesystem",
    "section": "5.7 Moving Things Around",
    "text": "5.7 Moving Things Around\nA lot of the time, we need to move files between shared filesystems. One filesystem might be good at storage and be backed up on a regular basis, while another filesystem might be better for temporary work on the cluster.\nYou might be familiar with mv, which lets you move files around in Unix. One thing to keep in mind when you’re mving things to a new folder that there is a difference between:\nmv log.txt my_folder   ## renames log.txt to my_folder\nand\nmv log.txt my_folder/  ## moves log.txt to be in my_folder\nThis is one thing that trips me up all the time.\nThis is one situation where using a GUI such as Motuz (Section 5.3.3) can be very helpful. You don’t have to worry about accidentally renaming files.\nOther tools for sync’ing between filesystems include rsync, which requires careful reading of documentation.\n\n\n\n\n\n\nThings I always forget: the difference between /home/mydir/ and home/mydir/\n\n\n\nSome things that trip me up all the time. The difference between\n/home/mydir/    #absolute path\nand\nhome/mydir/    #relative path\nThe first one is an absolute path, and the second is a relative path. Your clue is the leading / at the beginning of a path. If you’re getting file not found messages, check to make sure the path is the right format.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "filesystems.html#sec-paths",
    "href": "filesystems.html#sec-paths",
    "title": "5  Navigating the Unix Filesystem",
    "section": "5.5 Absolute versus relative paths",
    "text": "5.5 Absolute versus relative paths\nAbsolute paths contain all the information needed to find a file in a file system from the root / directory. For example, this would be an absolute path:\n/fh/fast/laderast/immuno_project/raw_data/chr2.fa.gz\nIn terms of folder structure, this is what this looks like:\n/\n├── fh\n│   └──fast\n│       └──laderast\n|            └──immuno_project\n│                 └──raw_data\n│                    └──chr2.fa.gz\nAbsolute paths always start with /, because that is the root directory, where all the top folders and files live.\nRelative paths break up an absolute path into two pieces of information: 1) your current directory and 2) the path relative to that directory. Relative paths are really helpful because things don’t break when you move your folder or files.\nIf my current working directory is the directory /fh/fast/laderas_t/immuno_project/, then the relative path to that same file would be:\nraw_data/chr2.fa.gz\nWe can visualize the relative path like this, where our working directory is indicated by a star\n/\n├── fh/fast/laderast/immuno_project/ *\n|      ^working directory        └──raw_data\n│                                   └──chr2.fa.gz\n                                    ^relative path\nNote that this relative path does not start with a /, because our current directory isn’t the root directory. Relative paths are incredibly useful when scripting in a reproducible manner, such as using project-based workflows to process files in a single folder.\n\n5.5.1 Keep Everything in Folders\nWe need to talk about code and data organization. For the FH system, we have a /home/ directory, and if we have generated research data, a /fh/fast/ directory. If we want our scripts to live in /home/ and our data is in /fh/temp/, we’ll need to refer to each of these file locations.\nIdeally, we want to make the naming conventions of our code and our data as similar as possible.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "intro-unix.html#hostname-what-machine-am-i-on",
    "href": "intro-unix.html#hostname-what-machine-am-i-on",
    "title": "4  Everything about Unix/Linux they didn’t teach you",
    "section": "4.3 hostname: What machine am I on?",
    "text": "4.3 hostname: What machine am I on?\nOne of the most confusing things about working on HPC is that sometimes you have a shell open on the head node, but oftentimes, you are on a worker node.\nYour totem for telling which node you’re in is hostname, which will give you the host name of the machine you’re on.\nFor example, if I used grabnode to grab a gizmo node for interactive work, I can check which node I’m in by using:\nhostname\ngizmok164\nIf you’re confused about which node you’re in, remember hostname. It will save you from making mistakes, especially when using utilities like screen.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "running_containers.html#testing-outside-of-the-container",
    "href": "running_containers.html#testing-outside-of-the-container",
    "title": "10  Working with containers",
    "section": "10.5 Testing outside of the container",
    "text": "10.5 Testing outside of the container\nLet’s take everything that we learned and put it in a script that we can run on the HPC:\n# Script to samtools view -c an input file:\n# Usage: ./run_sam.sh &lt;my_bam_file.bam&gt;\n# Outputs a count file: my_bam_file.bam.counts.txt\n#!/bin/bash\nmodule load Apptainer/1.1.6\napptainer run --bind /fh/fast/mydata:/mydata docker://biocontainers/samtools:v1.9-4-deb_cv1 samtools view -c /mydata/$1 &gt; /mydata/$1.counts.txt\n#apptainer cache clean\nmodule purge\nWe can use this script by the following command:\n./run_sam.sh chr1.bam \nAnd it will output a file called chr1.bam.counts.txt.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with containers</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#testing-your-scripts",
    "href": "interactive_shell.html#testing-your-scripts",
    "title": "7  Interactively Testing Scripts in Nodes",
    "section": "7.4 Testing Your Scripts",
    "text": "7.4 Testing Your Scripts\nPart of the reason for interactive testing is to make sure that the worker node can “see” and access all of your resources, including files and executables.\nThis is especially helpful when you are testing individual steps of a WDL workflow to make sure they work.\n\n\n\n\n\n\nWorking Interactively\n\n\n\nMany people will request a node and do their interactive analysis completely on that node. This can work great, but if you are requesting a huge allocation (such as a 64-core machine), you may be waiting a while to even get access to that allocation.\nIf you do the work of breaking up your scripts into WDL tasks, that makes utilizing the smaller spec’d machines for tasks like alignment much faster than waiting for a large allocation.\nBelieve me, it’s worth doing this.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactively Testing Scripts in Nodes</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#exit-when-done",
    "href": "interactive_shell.html#exit-when-done",
    "title": "7  Interactively Testing Scripts in Nodes",
    "section": "7.5 exit when done",
    "text": "7.5 exit when done\nWhen you’re done with the worker, you can exit the shell. This will put you back into the root node.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactively Testing Scripts in Nodes</span>"
    ]
  },
  {
    "objectID": "interactive_shell.html#screen-for-keeping-your-shell-open",
    "href": "interactive_shell.html#screen-for-keeping-your-shell-open",
    "title": "7  Interactively Testing Scripts in Nodes",
    "section": "7.6 screen for keeping your shell open",
    "text": "7.6 screen for keeping your shell open\nRemember the screen utility? (Section 3.6.2). You can utilize screen to keep your shell open on a remote worker. This is necessary when running interactive jobs when not connected.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Interactively Testing Scripts in Nodes</span>"
    ]
  },
  {
    "objectID": "hpc-safety.html",
    "href": "hpc-safety.html",
    "title": "3  HPC Security and Safety",
    "section": "",
    "text": "3.1 Learning Objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Security and Safety</span>"
    ]
  },
  {
    "objectID": "hpc-safety.html#learning-objectives",
    "href": "hpc-safety.html#learning-objectives",
    "title": "3  HPC Security and Safety",
    "section": "",
    "text": "Explain institutional restrictions on what data can be used in an HPC environment\nDecide whether your restricted use data meets the conditions for use in an HPC environment\nDocument your decision to use your data in an HPC environment",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Security and Safety</span>"
    ]
  },
  {
    "objectID": "hpc-safety.html#section",
    "href": "hpc-safety.html#section",
    "title": "3  HPC Security and Safety",
    "section": "3.2 ",
    "text": "3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Security and Safety</span>"
    ]
  },
  {
    "objectID": "container-concepts.html",
    "href": "container-concepts.html",
    "title": "9  Containers and Reproducibility",
    "section": "",
    "text": "9.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Containers and Reproducibility</span>"
    ]
  },
  {
    "objectID": "container-concepts.html#learning-objectives",
    "href": "container-concepts.html#learning-objectives",
    "title": "9  Containers and Reproducibility",
    "section": "",
    "text": "Explain the benefits of using containers for reproducibility and for batch processing\nDefine the terms image, container, and snapshot in the context of Docker",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Containers and Reproducibility</span>"
    ]
  },
  {
    "objectID": "container-concepts.html#why-containers",
    "href": "container-concepts.html#why-containers",
    "title": "9  Containers and Reproducibility",
    "section": "9.2 Why Containers?",
    "text": "9.2 Why Containers?\nThere is a replication crisis out there. Even given a script and the raw data, it is often difficult to replicate the results generated by a study.\nWhy is this difficult? One reason is that the results are tied to software and database versions.\nThis is one motivation for using containers - they are a way of packaging software that ‘freezes’ the software versions. It allows you to recreate the software environment associated with running your software,\nAdditionally, containers are portable - they function as mini-operating systems that can run on any machine that runs your container software. other people should be able to replicate your results even if they’re on a different operating system.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Containers and Reproducibility</span>"
    ]
  },
  {
    "objectID": "container-concepts.html#terminology",
    "href": "container-concepts.html#terminology",
    "title": "9  Containers and Reproducibility",
    "section": "9.3 Terminology",
    "text": "9.3 Terminology\nIn order to be unambiguous with our language, we’ll use the following definitions:\n\n\n\n\n\n\nFigure 9.1: Docker Terms\n\n\n\n\nRegistry - collection of repositories that you pull docker images from. Example repositories include DockerHub and Quay.io.\nDocker Image - what you download from a registry - the “recipe” for building the software environment. Stored in a registry. use docker pull to get image, docker commit to push changes to registry, can also generate image from a Dockerfile,\nDocker Container - The executable software environment actually installed and running on a machine. Runnable. Generate from docker pull from a repository.\nSnapshot File - An single archive file (.tar.gz) that contains the Docker container. Generate using docker save on a container. Also known as an image file on the platform.\n\n\n9.3.1 Be Secure\nBefore we get started, security is always a concern when running containers. The docker group has elevated status on a system, so we need to be careful that when we’re running them, they aren’t introducing any system vulnerabilities.\nThese are mostly important when running containers that are web-servers or part of a web stack, but it is also important to think about when running jobs on HPC.\nHere are some guidelines to think about when you are working with a container.\n\nUse vendor-specific Docker Images when possible.\nUse container scanners to spot potential vulnerabilities. DockerHub has a vulnerability scanner that scans your Docker images for potential vulnerabilities.\nAvoid kitchen-sink images. One issue is when an image is built on top of many other images. It makes it really difficult to plug vulnerabilities. When in doubt, use images from trusted people and organizations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Containers and Reproducibility</span>"
    ]
  },
  {
    "objectID": "container-concepts.html#making-dockerfiles",
    "href": "container-concepts.html#making-dockerfiles",
    "title": "9  Containers and Reproducibility",
    "section": "9.4 Making Dockerfiles",
    "text": "9.4 Making Dockerfiles\nThe other way to build image files is to use a Dockerfile. A Dockerfile is a recipe for installing software and its dependencies.\nLet’s take a look at a Dockerfile. By default, it is contained within a folder and is called Dockerfile:\nFROM ubuntu:18.04\n\nRUN apt-get update && \\\n    apt-get install -y build-essential  && \\\n    apt-get install -y wget && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\nENV CONDA_DIR /opt/conda\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh && \\\n     /bin/bash ~/miniconda.sh -b -p /opt/conda\n\nENV PATH=$CONDA_DIR/bin:$PATH\n\n#install plink with conda\nRUN conda install -c \"bioconda/label/cf201901\" plink\nRUN conda install -c \"bioconda/label/cf201901\" samtools\nWe can build the Docker image in our directory using:\ndocker build . -t gatk_sam_plink:0.0.1\nWhen it’s done, we can then make sure it’s been built by using\ndocker images\nAnd we can use it like any other image.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Containers and Reproducibility</span>"
    ]
  },
  {
    "objectID": "container-concepts.html#going-further-with-docker",
    "href": "container-concepts.html#going-further-with-docker",
    "title": "9  Containers and Reproducibility",
    "section": "9.5 Going Further with Docker",
    "text": "9.5 Going Further with Docker\nNow that you know how to build a snapshot file, you’ve also learned another step in building apps: specifying software dependencies. You can use these snapshot files to specify executables in your app.\nYou can also use these snapshot files in your WDL workflow.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Containers and Reproducibility</span>"
    ]
  },
  {
    "objectID": "container-concepts.html#what-you-learned-in-this-chapter",
    "href": "container-concepts.html#what-you-learned-in-this-chapter",
    "title": "9  Containers and Reproducibility",
    "section": "9.6 What you learned in this chapter",
    "text": "9.6 What you learned in this chapter\n\nHow containers enable reproducibility\nDefined specific container terminology",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Containers and Reproducibility</span>"
    ]
  },
  {
    "objectID": "hpc-safety.html#what-are-the-regulations-associated-with-using-hpc-with-data",
    "href": "hpc-safety.html#what-are-the-regulations-associated-with-using-hpc-with-data",
    "title": "3  HPC Security and Safety",
    "section": "3.2 What are the regulations associated with using HPC with data?",
    "text": "3.2 What are the regulations associated with using HPC with data?\nhttps://csrc.nist.gov/projects/cprt/catalog#/cprt/framework/version/SP_800_53_5_1_1/home?element=AT-01",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Security and Safety</span>"
    ]
  },
  {
    "objectID": "hpc-safety.html#be-safe-with-restricted-use-data",
    "href": "hpc-safety.html#be-safe-with-restricted-use-data",
    "title": "3  HPC Security and Safety",
    "section": "3.3 Be Safe with Restricted Use Data",
    "text": "3.3 Be Safe with Restricted Use Data\nSubject privacy is extremely important.\n\nUse the Safe Harbor Method for de-identifying your clinical data.\nUse deidentified versions of the patient data on HPC. When in doubt, use de-identified data for processing data. Most of the applications to HPC will be processing genomic or other datatypes.\nUse only the covariates necessary in your study. For example, for a GWAS study of type II diabetes, bring in\nIf using data under a Data Use Agreement (DUA), make sure you are comfortable with the terms. Many DUAS",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Security and Safety</span>"
    ]
  },
  {
    "objectID": "index.html#other-resources",
    "href": "index.html#other-resources",
    "title": "Bash for HPC",
    "section": "1.5 Other Resources",
    "text": "1.5 Other Resources\nWe recommend reviewing a course such as the Software Carpentry course for Shell Scripting before getting started with this book. The Missing Semester of your CS Education is another great introduction/resource.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "hpc-safety.html#what-are-the-regulations-associated-with-using-hpc-with-restricted-use-data",
    "href": "hpc-safety.html#what-are-the-regulations-associated-with-using-hpc-with-restricted-use-data",
    "title": "3  HPC Security and Safety",
    "section": "3.2 What are the regulations associated with using HPC with restricted use data?",
    "text": "3.2 What are the regulations associated with using HPC with restricted use data?\n\nhttps://csrc.nist.gov/projects/cprt/catalog#/cprt/framework/version/SP_800_53_5_1_1/home?element=AT-01\nhttps://www.nrel.gov/hpc/data-security-policy.html\nhttps://ipo.llnl.gov/technologies/it-and-communications/processing-protected-data-high-performance-computing-clusters\nDHS Decision Charts",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Security and Safety</span>"
    ]
  },
  {
    "objectID": "filesystems.html#whats-next",
    "href": "filesystems.html#whats-next",
    "title": "5  Navigating the Unix Filesystem",
    "section": "5.8 What’s Next?",
    "text": "5.8 What’s Next?\nNow that we have learned the different filesystems and how to specify them and navigate them, we are now able to try out shell scripting (Chapter 7).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Navigating the Unix Filesystem</span>"
    ]
  },
  {
    "objectID": "intro-unix.html#sec-environment",
    "href": "intro-unix.html#sec-environment",
    "title": "4  Everything about Unix/Linux they didn’t teach you",
    "section": "4.4 Environment Variables",
    "text": "4.4 Environment Variables\nEnvironment variables are variables which can be seen globally in the Linux (or Windows) system across executables.\nYou can get a list of all set environment variables by using the env command. Here’s an example from my own system:\nenv\nSHELL=/bin/bash\nNVM_INC=/home/tladera2/.nvm/versions/node/v21.7.1/include/node\nWSL_DISTRO_NAME=Ubuntu\nNAME=2QM6TV3\nPWD=/home/tladera2\nLOGNAME=tladera2\n[....]\nOne common environment variable you may have seen is $JAVA_HOME, which is used to find the Java Software Development Kit (SDK). (I usually encounter it when a software application yells at me when I haven’t set it.)\nYou can see whether an environment variable is set using echo, such as\necho $PATH\n/home/tladera2/.local/bin:/home/tladera2/gems/bin:/home/tladera2/.nvm/versions/node/v21.7.1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/ [....]\n\n\n\n\n\n\nEnvironment Variables in Windows\n\n\n\nSince we’re mostly going to be working in a Unix environment, we’re not going to touch on Windows environment variables. However, there is also a $PATH environment variable that you can set.\nI recommend looking at the PowerShell documentation for more information about Windows-specific environment variables\n\n\n\n4.4.1 Setting Environment Variables\nIn Bash, we use the export command to declare an environment variable. For example, if we wanted to declare the environment variable $SAMTOOLS_PATH we’d do the following:\n# works: note no spaces\nexport SAMTOOLS_PATH=\"/home/tladera2/miniconda/bin/\"\nOne thing to note is that spacing matters when you declare environment variables. For example, this won’t declare the $SAMTOOLS_PATH variable:\n# won't work because of spaces\nexport SAMTOOLS_PATH = \"/home/tladera2/miniconda/bin/\"\nAnother thing to note is that we declare environment variables differently than we use them. If we wanted to use SAMTOOLS_PATH in a script, we use a dollar sign ($) in front of it:\n$SAMTOOLS_PATH/samtools view -c $input_file\nIn this case, the value of $SAMTOOLS_PATH will be expanded (substituted) to give the overall path:\n/home/tladera2/miniconda/bin/samtools view -c $input_file\n\n\n4.4.2 A Very Special Environment Variable: $PATH\nThe most important environment variable is the $PATH variable. This variable is important because it determines where to search for software executables (also called binaries). If you have softwware installed by a package manager (such as miniconda), you may need to add the location of your executables to your $PATH.\nWe can add more directories to the $PATH by appending to it. You might have seen the following bit of code in your .bashrc:\nexport PATH=$PATH:/home/tladera2/samtools/\nIn this line, we are adding the path /home/tladera2/samtools/ to our $PATH environment variable. Note that how we refer to the PATH variable is different depending on which side the variable is on of the equals sign.\nTLDR: We declare the variable using export PATH (no dollar sign) and we append to the variable using $PATH (with dollar sign). This is something that trips me up all the time.\n\n\n\n\n\n\nFor FH Users\n\n\n\nIn general, when you use environment modules on gizmo, you do not need to modify your $PATH variable. You mostly need to modify it when you are compiling executables so that the system can find them. Be sure to use which to see where the environment module is actually located:\nwhich samtools\n\n\n\n\n4.4.3 Making your own environment variables\nOne of the difficulties with working on a cluster is that your scripts may be in one filesystem (/home/), and your data might be in another filesystem (/fh/fast/). And it might be recommended that you transfer over files to a faster-access filesystem (/fh/temp/) to process them.\nYou can set your own environment variables for use in your own scripts. For example, we might define a $TCR_FILE_HOME variable:\nexport TCR_FILE_HOME=/fh/fast/my_tcr_project/\nto save us some typing across our scripts. We can use this new environment variable like any other existing environment variable:\n#!/bin/Bash\nexport my_file_location=$TCR_FILE_HOME/fasta_files/\n\n\n4.4.4 .bashrc versus .bash_profile\nOk, what’s the difference between .bashrc and .bash_profile?\nThe main difference is when these two files are sourced. bash_profile is used when you do an interactive login, and .bashrc is used for non-interactive shells.\n.bashrc should contain the environment variables that you use all the time, such as $PATH and $JAVA_HOME for example.\nYou can get the best of both worlds by including the following line in your .bash_profile:\nsource ~/.bashrc\nThat way, everything in the .bashrc file is loaded when you log in interactively.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Everything about Unix/Linux they didn't teach you</span>"
    ]
  },
  {
    "objectID": "running_containers.html#apptainer-cache",
    "href": "running_containers.html#apptainer-cache",
    "title": "10  Working with containers",
    "section": "10.6 Apptainer Cache",
    "text": "10.6 Apptainer Cache\nThe apptainer cache is where your docker images live. They are translated to the native apptainer .sif format.\nYou can see what’s in your cache by using\napptainer cache list\nBy default the cache lives at ~/.apptainer/cache.\nIf you need to clear out the cache, you can run\napptainer cache clean\nto clear out the cache.\nThere are a number of environment variables (Section 4.4) that can be set, including login tokens for pulling from a private registry. More information is here.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with containers</span>"
    ]
  }
]