# HPC Basics {#sec-cloud} 

We all need to start somewhere when we work with High Performance Computing (HPC). 

This chapter is a review of how HPC works and the basic concepts. If you haven't used HPC before, no worries! This chapter will get you up to speed.

## Learning Objectives

After reading this chapter, you should be able to:

1. **Define** key players in both local computing and HPC
1. **Articulate** key differences between local computing and HPC
1. **Describe** the sequence of events in launching jobs in the DNAnexus cloud
1. **Differentiate** local storage from shared storage

## Important Terminology

Let's establish the terminology we need to talk about HPC computing.

- **Toolchain** - a piece of software and its dependencies needed to run on a computer. For example, `cromwell` (a workflow runner), and `java`.
- **Software Environment** - everything needed to run a piece of software on a brand new computer. For example, this would include installing R, but also all of its dependencies as well. 
- **Executable** - software on the HPC. 
- **Shared Filesystem** - Part of the platform that stores our files and other objects. We'll see that these other objects include applets, databases, and other object types.

## Understanding the key players

In order to understand what's going on with HPC, we will have to change our mental model of computing. 

Let's contrast the key players in local computing with the key players in HPC.

### Key Players in Local Computing

![Local Computing](images/local_compute.png){#fig-local}

- Our Machine

When we run an analysis or process files on our computer, we are in control of all aspects of our computer. We are able to install a software environment, such as R or Python, and then execute scripts/notebooks that reside on our computer on data that's on our computer.

Our main point of access to either the HPC cluster is going to be our computer.


### Key Players in HPC

Let's contrast our view of local computing with the key players in the DNAnexus platform (@fig-dnanexus-players).

![Key Players in HPC](images/hpc_architecture.png){#fig-hpc-architecture}

- **Our Machine** - We interact with the platform via the dx-toolkit installed on our machine. When we utilize cloud resources, we request them from our own computer using commands from the dx toolkit.
- **Head Node** - Although there are many parts, we can treat the DNAnexus platform as a single entity that we interact with. Our request gets sent to the platform, and given availability, it will grant access to a temporary DNAnexus Worker. Also contains **project storage**.
- **Worker Node** - A temporary machine that comes from a pool of available machines. We'll see that it starts out as a blank slate.
- **Shared Filesystem** A distributed filesystem that can be seen by all of the nodes in the cluster.
- **SLURM** - The workload manager of the HPC. Commands in **SLURM** such as `srun`, (see below) kick off the processes on executing jobs on the **worker nodes**
- **Interactive Analysis** - Any analysis that requires interactive input from the user. Using RStudio and JupyterLab are two examples of interactive analysis. As opposed to *non-interactive* analysis, which is done via scripts.

:::{.callout-note}
## For Fred Hutch Users

The `gizmo` cluster at Fred Hutch actually has 3 head nodes called `rhino` (`rhino01`, `rhino02`, `rhino03`) that are high spec machines (70+ cores, lots of memory). You can run jobs on these nodes, but be aware that others may be running jobs here as well.

The nodes from `gizmo` all have names like `gizmoj6`, depending on their architecture.
:::


## Sequence of Events of Running a Job

Let's run through the order of operations of running a job on the platform. Let's focus on running an aligner (BWA-MEM) on a FASTQ file. Our output will be a .BAM (aligned reads) file.

Let's go over the order of operations needed to execute our job on the HPC cluster (@fig-cluster).

![Order of Operations](images/hpc_process.png){#fig-cluster} 

A. **Start a job using `srun` to send a request to the platform.** In order to start a job, we will need two things: software (`samtools`), and a file to process from the shared filesystem (not shown). When we use `srun`, a request is sent to the platform.
B. **Platform requests for a worker from available workers; worker made available on platform.** In this step, the head node looks for a worker instance that can meet our needs. Then the computations run on worker; output files are generated.** Once our app is ready and our file is transferred, we can run the computation on the worker.
C. **Output files transferred back to project storage.** Any files that we generate during our computation (`53525342.bam`) must be transferred back into the shared filesystem.

When you are working with an HPC cluster, especially with batch jobs, keep in mind this order of execution. Being familiar with how the key players interact on the platform is key to running efficient jobs. 

## Key Differences with local computing

As you might have surmised, running a job on the DNAnexus platform is very different from computing on your local computer.

1. We don't own the worker machine, we only have temporary access to it. A lot of the complications of running cloud computations comes from this.
2. We have to be explicit what kind of machine we want. We'll talk much more about this in terms of machine types.

### The Shared Filesystem

Clusters often have a shared filesystem to make things easier. These filesystems can be accessed by all the nodes in a cluster and are designed for fast file transfers and reading. One example of a filesystem is Lustre. 

Think about that: it's like having an SSD attached to all of the nodes. But how does the storage work?

The filesystem is distributed such that each set of nodes has a relatively fast access to the files on the system. The data itself is *sharded*, or broken up, and distributed among the storage servers that provide access to the files.




